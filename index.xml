<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello world!</title>
    <link>https://shiweiHou.github.io/</link>
    <description>Recent content on Hello world!</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 25 Nov 2016 20:52:54 +0800</lastBuildDate>
    <atom:link href="https://shiweiHou.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ex6.Support Vector Machines</title>
      <link>https://shiweihou.github.io/machinelearning/ex6/</link>
      <pubDate>Fri, 25 Nov 2016 20:52:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex6/</guid>
      <description>

&lt;p&gt;这节讲到了SVM，Andrew Ng大神主要简单介绍了下核函数和SVM一些参数的影响。因为Ng大牛介绍的比较简单，所以看完自己又去网上百度了下，系统了了解了下。拙见就不在这发了，大家有兴趣的可以自己去百度。其实SVM就是个分类器，利用特定的核函数将在原空间维度线性不可分问题映射到高维，使之变成线性可分。另外学习这节课的时候又对偏差和方差有点迷，在知乎上找到一篇答案，比较优秀，链接在此：&lt;a href=&#34;https://www.zhihu.com/question/27068705&#34;&gt;机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;gaussian-kernel&#34;&gt;Gaussian Kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%很简单，就是按照公式计算高斯核内积，难度0
sim = -sum( (x1 - x2).^2 ) /  2 / sigma^2;
sim = exp(sim);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dataset3params-m&#34;&gt;dataset3Params.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%选择出最好的一对 C sigma
C_final = C;
sigma_final = sigma;
min_error = size(yval,1);
% C 和 sigma 所有的可能
para = [0.01 0.03 0.1 0.3 1 3 10 30];
m = size(para,2);
% 将 C 和 sigma 所有的可能都训练测试一遍，找到效果最好的那对
for i = 1 : m
    for j = 1 : m
        C = para(i);
        sigma = para(j);
        model = svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));
        pre = svmPredict(model,Xval);
        cur_error = mean(double(pre ~= yval));
        if cur_error &amp;lt; min_error
            min_error = cur_error;
            C_final = C;
            sigma_final = sigma;
        end
    end
end
C = C_final;
sigma = sigma_final;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;processemail-m&#34;&gt;processEmail.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%就是找到单词出现的下标位置，然后已列向量存储下来
for i = 1 : length(vocabList)
    if strcmp(str, vocabList{i}) == 1
        word_indices = [word_indices ; i];
        break;
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;emailfeatures-m&#34;&gt;emailFeatures.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%这个就更简单了，简单的说就是将所有出现的单词所在的下标位置，赋值1
for i = 1 : size(word_indices,1)
    x(word_indices(i)) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在已经学习到第七周了，发现越高深的算法，其实实现起来越简单，因为现有的函数库都将其实质内容都包含隐藏起来了，你只需要实现相应的输入参数就可以了。说明发明一个算法是最难的，但学会用一个算法，就太简单了。
另在原有的代码有一个小bug，就是没办法显示Example Dataset 2的边界线，是visualizeBoundary.m出了点问题，课程助教也给了解决方法，及时将原有的&lt;code&gt;contour(X1, X2, vals, [0 0], &#39;Color&#39;, &#39;b&#39;);&lt;/code&gt;改为&lt;code&gt;contour(X1, X2, vals, [1 1], &#39;b&#39;);&lt;/code&gt;就可以了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pcl_boundary</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_boundary/</link>
      <pubDate>Fri, 18 Nov 2016 13:41:18 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_boundary/</guid>
      <description>&lt;p&gt;PCL 里又找到一个牛逼的函数：BoundaryEstimation，用来发现三维点云的边界的。因为一直在做三维重建，一开始的想法是将三维点云投影到二维上，接着将二维的点云进行网格划分，根据落在网格内的点的数量设置为灰度值，这样我们就可以得到一个灰度图。接着就是利用图像处理中的方法，先进行二值化，然后用canny算子找边界，用hough变换找直线，最后再反投射到三维上。后来试了下，这个方法不仅复杂，而且进行两次三维到二维之间的变化，误差太大。后来在PCL里找到BoundaryEstimation这个类，能够自动的找到边界，很强大，但最好也是在二维下做，意思是先找到面，在面上找边界。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void LasFunction::boundaryFind(void) {
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();
    pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt;::Ptr tree = boost::shared_ptr&amp;lt;pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt; &amp;gt;(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;);
    // 计算法向量，因为边界点查找需要用到法向量
    pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;::Ptr normals(new pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;);
    pcl::NormalEstimation&amp;lt;pcl::PointXYZ, pcl::Normal&amp;gt; normal_estimator;
    normal_estimator.setSearchMethod(tree);
    normal_estimator.setInputCloud(cloud);
    normal_estimator.setKSearch(30);
    normal_estimator.compute(*normals);

    pcl::PointCloud&amp;lt;pcl::Boundary&amp;gt; boundaries;
    pcl::BoundaryEstimation&amp;lt;pcl::PointXYZ, pcl::Normal, pcl::Boundary&amp;gt; est;
    est.setInputCloud(cloud);
    est.setInputNormals(normals);
    est.setSearchMethod(pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;::Ptr(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;));
    //50效果比较好，这个数值的意思我的理解是根据点周围这么多个点来进行判断我们找到的点究竟是不是边界点
    est.setKSearch(100);
    est.compute(boundaries);
    if (boundaries.points.empty()) std::cout &amp;lt;&amp;lt; &amp;quot;error&amp;quot; &amp;lt;&amp;lt; std::endl;
    else {
        pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr boundaryPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
        for (int i = 0; i &amp;lt; cloud-&amp;gt;points.size(); ++i) {
        //当初在这卡了好久，因为虽然找到了边界点，但无法直接输出，它里面是uint8_t类型的，如果是边界点，它保存1，如果不是，保存0.若想在计算机里显式的显示出来，需要进行强制类型转换，变成int型才可以
            uint8_t  x = boundaries.points[i].boundary_point;
            int a = static_cast&amp;lt;int&amp;gt;(x);
            if (1 == a) boundaryPoints-&amp;gt;push_back(cloud-&amp;gt;points[i]);
        }
        std::cout &amp;lt;&amp;lt; boundaryPoints-&amp;gt;points.size() &amp;lt;&amp;lt; std::endl;
        std::cout &amp;lt;&amp;lt; &amp;quot;boundary is ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        radiusFilter(boundaryPoints);
        pcl::io::savePCDFile(&amp;quot;./line9/boundaryPlane.pcd&amp;quot;, *boundaryPoints);
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总之，这个类给了我很大的方便，PCL果然在一直进步，很牛逼的一个库，需要我继续学习下去&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PCL SampleConsensusMode</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_ransac2/</link>
      <pubDate>Fri, 18 Nov 2016 13:19:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_ransac2/</guid>
      <description>

&lt;p&gt;前面那篇用PCL里的ransac模型进行面提取，其实严格意义来讲并不是真正的RANSAC类，而是利用的SACSegmentation类，然后在类。里面用SACMODEL_PLANE模型进行面提取。后来又找到PCL里真正的RANSAC函数 SampleConsensusMode，遂用这个实现了一下，发现效果要比前面的好一些，并且不仅在提面上效果好，在提线的表现上也非常好。&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelplane&#34;&gt;SampleConsensusModelPlane&lt;/h2&gt;

&lt;p&gt;这个用来提取面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelPlane(void) {
    // ransac 提取出来一个最大的平面
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    // 读取文件，将点云数据放入cloud中
    cloud = readFile();
    // 原始点云数量，用来判断何时停止面的提取
    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    // 当剩下的点云数量比原始点云数量大于10%时时，继续进行面的提取
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.1 * rawPoints) {

        //确定提取模型为面，点云类型要和前面的点云类型一致
        pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_p(new pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_p);
        //这个Threshold的意思是，同一平面的点相邻距离不能超过0.1m，或者是已0.1m为基准进行搜索，不能超过它。我试过用1 2等参数进行提取，发现提取出来的不仅仅是一个面了，还有高度，就像是一个立方体一样。所以最好小一些，可以根据具体的点云数据进行设置
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        //这个函数的意思是将inputPoints中，前面提取到的面（索引保留在了inliers中了），复制到点云out中，这时out中存的就是前面提取的面的点
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //建立析取器，因为我们需要不停的迭代，需要将上次提取出来的面从原来的点云中去除
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        //因为前面用到的索引是vector类型的，而析取器要求的是PointIndices，所以只要新建个，然后将上面的索引复制给它就行了
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        //只有设置为true，才可以将包含在inlier中的下标的点从原来的点云中去除
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);

        std::cout &amp;lt;&amp;lt; &amp;quot;ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;SampleModelPlane&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以提取面之外，这个函数还有个变形，就是提取直线，效果也是非常好的&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelline&#34;&gt;SampleConsensusModelLine&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelLine(void) {
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();

    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    std::multimap&amp;lt;double, double&amp;gt; mapKb;
    //和上面的函数几乎没有什么区别，只是将SampleConsensusModelPlane变成SampleConsensusModelLine
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.05 * rawPoints) {
        pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_l(new pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_l);
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //析取
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);


        std::cout &amp;lt;&amp;lt; &amp;quot;ok2.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;./line1/SampleModelLine&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }

    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为用的是1.8版本的PCL，里面实现了好多很需要的函数，所以功能还是很强大，几乎你能想到的都有，只是因为是通用的，所以可能在精度准确度上没有那么高，需要开发者自己再做进一步调整&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex5：Regularized Linear Regression and Bias v.s.Variance</title>
      <link>https://shiweihou.github.io/machinelearning/ex5/</link>
      <pubDate>Thu, 17 Nov 2016 21:15:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex5/</guid>
      <description>

&lt;p&gt;这个练习其实很简单，基本上都是以前练习的代码，然后让你再重写一遍。这次编程作业的主要作用就是让我们直观的看到不同大小的训练集及不同大小的lambda对拟合效果的影响。具体有以下几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature太少或者维数过低，会造成高偏差(high bias)，即欠拟合,解决方法是增加feature数，可以是维数增加或者feature数增加（polynomial features)或者降低lambda（正则化项）,feature数增加的前提示增加的feature必须和已有feature是相互独立的，不然没有任何意义，只能增加维数了&lt;/li&gt;
&lt;li&gt;类似于上条，如果feature过多或者feature的维数过高，会造成高方差(high variance),即过拟合。解决方法是减少feature数，将不是相互独立的feature去掉，或者降低维数，或者增加正则化项，降低维数对theta参数的影响。还有一个很有用的方法是增加训练集样本，来解决过拟合问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;regularized-linear-regression-cost-function-and-gradient&#34;&gt;Regularized linear regression cost function and Gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%非矩阵形式
for i = 1 : m
    h = theta(1) * X(i,1) + theta(2) * X(i,2);
    J = J +  (h - y(i))^2 / 2 / m;
end
J = J + lambda / 2 / m * theta(2)^2;
%矩阵形式
J = sum((X * theta - y).^2) / 2 / m + lambda / 2 /m * (sum(theta.^2) - theta(1)^2); 

theta1 = theta;
theta1(1) = 0;
grad = (X&#39; * (X * theta - y))/m + lambda / m * theta1;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;learning-curves&#34;&gt;Learning curves&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% ---------------------- Sample Solution ----------------------
for i = 1 : m
    theta = trainLinearReg(X(1:i,:), y(1:i,:), lambda);
    [error_train(i),grad] = linearRegCostFunction(X(1:i,:), y(1:i,:), theta, 0);
    [error_val(i),grad] = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个步骤中变得很笨，这个步骤是要让我们形式化的看到不同的训练集大小对误差的影响，我在计算训练误差的时候还好，但在计算error_val交叉集误差的时候，也学训练集那样只用Xval和yval的前i个样本，其实是要全部的样本。&lt;/p&gt;

&lt;h2 id=&#34;polynomial-regression&#34;&gt;Polynomial regression&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : p
    X_poly(:,i) = X.^i;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分主要让我们观察不同的feature polynomial对误差的影响，意思是原来的训练集X只有一列，然后我们根据p的值，另外增加p-1列，其中第2列的值为第一列值的平方，第3列的值为第一列值的立方，以此类推&lt;/p&gt;

&lt;h2 id=&#34;selecting-λ-using-a-cross-validation-set&#34;&gt;Selecting λ using a cross validation set&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : length(lambda_vec)
    lambda = lambda_vec(i);
    theta = trainLinearReg(X, y, lambda);
    error_train(i) = linearRegCostFunction(X, y, theta, 0);
    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分就和Learning curves这部分很像，所不同的是Learning curves部分看训练集大小对误差的影响，这次呢，是看不同的lambda λ对误差的影响，训练集和交叉集大小是固定的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks Learning</title>
      <link>https://shiweihou.github.io/machinelearning/ex4/</link>
      <pubDate>Wed, 09 Nov 2016 10:49:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex4/</guid>
      <description>

&lt;p&gt;Neural Networks Learning：实现最简单的一个神经网络学习系统，实现反向传播和正向传播，并利用数值计算误差来检测神经网络算法是否可行，或者说cost function是否计算正确。&lt;/p&gt;

&lt;h2 id=&#34;feedforward-and-cost-function&#34;&gt;Feedforward and cost function&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through the
%               following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%         variable J. After implementing Part 1, you can verify that your
%         cost function computation is correct by verifying the cost
%         computed in ex4.m
a1 = [ones(m,1) X]; % 5000 * 401
Z2 = a1 * Theta1&#39;;  % 5000 * hidden_layer
a2 = sigmoid(Z2);   % sigmoid
a2 = [ones(size(a2,1),1) a2]; % 5000 * (hidden_layer + 1)
Z3 = a2 * Theta2&#39;;            % 5000 * 10
a3 = sigmoid(Z3);
[max_a3, index] = max(a3,[],2);

I = eye(num_labels); 
Y = zeros(m, num_labels);  % 5000 * 10
for i = 1:m
    Y(i,:) = I(y(i),:);  %  让每一个样例第y(i)列变成1，即将样本输出变成行向量，而不是列向量，每一行为1的列的值即为样本的输出
end
%J = sum(sum((-Y).*log(a3) - (1-Y).*log(1-a3),2))/m;
JJ = 0;
for i = 1 : m   
   JJ = JJ + sum( -1*Y(i,:).* log(a3(i,:)) - (1-Y(i,:)).*log(1-a3(i,:))); %对所有的样本进行相加，每个样本计算J，最后加起来
end
J = JJ / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-cost-function&#34;&gt;Regularized cost function&lt;/h2&gt;

&lt;p&gt;就是在前面计算出来的J的基础上，加上正则化项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%要去掉theta(1),第一个theta不需要参加
theta1 = Theta1(:,2:size(Theta1,2));
theta2 = Theta2(:,2:size(Theta2,2));
reg = lambda * ( sum ( sum ( theta1.^2) ) + sum ( sum ( theta2.^2) ));
reg = reg / 2 / m;
J = J + reg;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sigmoid-gradient&#34;&gt;Sigmoid gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function g = sigmoidGradient(z)
%SIGMOIDGRADIENT returns the gradient of the sigmoid function
%evaluated at z
%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function
%   evaluated at z. This should work regardless if z is a matrix or a
%   vector. In particular, if z is a vector or matrix, you should return
%   the gradient for each element.

g = zeros(size(z));
g = sigmoid(z).* (1 - sigmoid(z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sigma3 = a3-Y;
sigma2 = (sigma3*Theta2).*sigmoidGradient([ones(size(Z2, 1), 1) Z2]);
sigma2 = sigma2(:, 2:end);

delta_1 = (sigma2&#39;*a1);
delta_2 = (sigma3&#39;*a2);

p1 = (lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];
p2 = (lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];
Theta1_grad = delta_1./m + p1;
Theta2_grad = delta_2./m + p2;

grad = [Theta1_grad(:) ; Theta2_grad(:)];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;神经网络算法比较简单，就是计算过程稍微复杂了点，特别是牵涉到里面的矩阵运算的时候，比较麻烦。第一次做的时候感觉无从下手，后来又做了一遍，才感觉稍微懂点。思路都懂，就是写出来比较困难，还需努力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-class Classification and Neural Networks</title>
      <link>https://shiweihou.github.io/machinelearning/ex3/</link>
      <pubDate>Thu, 03 Nov 2016 19:56:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex3/</guid>
      <description>

&lt;p&gt;Multi-class Classification&lt;/p&gt;

&lt;p&gt;多分类，其实和前面的二分类，即Logistics regression差不多，只不过因为分类结果有很多个，对于特定的输入，会产生多个输出，在这多个输出里面找到概率最大的那个输出，即为分类答案。&lt;/p&gt;

&lt;h2 id=&#34;lrcostfunction-m&#34;&gt;lrCostFunction.m&lt;/h2&gt;

&lt;p&gt;和第二个练习的一样，就是指导文档里要求我们向量化，即矩阵化求解。前面的练习我们已经做到了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J = (0-y).*log(J1) - (1-y).* log (1-J1);
J = sum(J);
J = J / m;
J = J + (lambda / 2 / m) * (sum(theta.^2) - theta(1)^2);
% 求 Gradient 
grad = X&#39; *  (J1 - y ) / m;
tmp = theta;
tmp(1) = 0;
grad = grad + (lambda / m) * tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;one-vs-all-classification&#34;&gt;One-vs-all Classification&lt;/h2&gt;

&lt;p&gt;其实多分类也是一个“二分类”问题，只不过每次我们都是选取其中概率最大的那个作为输出“1”，其余的都作为“0”。&lt;strong&gt;oneVsAll.m&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for c = 1 : num_labels
    %yc = (y == c);
    initial_theta = zeros(n + 1, 1);
    options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
    [theta] = ...
         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...
                 initial_theta, options);
    all_theta(c,:) = theta&#39;;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;predictonevsall-m&#34;&gt;predictOneVsAll.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% max函数，得到tmp矩阵中每一行最大的那个列下标
% 在本题中，列下标即为分类结果
[maximum, index] = max(tmp, [], 2);
% 如果下标是10，我们就用0表示
index(index == 10) = 0;
p = index;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;这节课Andrew还简单的介绍了下神经网络，并且给出了如何利用神经网络来分类。编程题中Andrew已经给好了训练好的Theta值，只需要我们能根据训练好的Theta值和输入的X值，得到Y值就行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
%加上第一项1（bias unit)，因为X是m * n的，每个样本的特征在一行，所以加上的就是 m*1的一列，即在每一行的前面加上了个1
X = [ones(m, 1) X];
for i = 1 : m
    XX = X(i:i,:);
    z2 = Theta1 * XX&#39;;
    z2 = sigmoid(z2);
    %row = size(z2,1)
    %加上第一项 1，因为z2是 n*1的，即第i个样本的z2都是单独一列，是个列向量，因此需要加上一行1，即增加一行1
    z22 = [ones(1,1);z2];
    z3 = Theta2 * z22;
    % a3 即为输出结果
    a3 = sigmoid(z3);
    [maxInd, index] = max(a3,[],1);
    if index == 0
        index = 10;
    end
    p(i) = index;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在每一层网络中计算的时候，都要注意要加上一项1作为bias unit&lt;/li&gt;
&lt;li&gt;注意在前面一层输出和Theta积计算完之后，不要忘记做sigmoid操作&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Logistics Regression</title>
      <link>https://shiweihou.github.io/machinelearning/ex2/</link>
      <pubDate>Thu, 03 Nov 2016 18:34:49 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex2/</guid>
      <description>

&lt;p&gt;第二次编程作业，逻辑回归，其实可以理解为二分类问题。&lt;/p&gt;

&lt;h2 id=&#34;sigmoid-function&#34;&gt;sigmoid function：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% 迭代实现
[row, col] = size(z); 
for i = 1:row
    for j = 1:col
        g(i,j) = 1 / (1 + exp(-z(i,j)));
    end
end

% 非迭代实现
g = 1.0 ./ (1.0 + exp(-z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cost-function-and-gradient&#34;&gt;Cost function and Gradient：&lt;/h2&gt;

&lt;p&gt;一开始我是用迭代做的，后来学习了第四课之后，我又回来将迭代的版本改为矩阵版本，看起来确实清晰了很多，以后尽量都用矩阵形式来做（好像这样也更符合matlab的要求）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 迭代版本
theta1 = theta&#39;;
[row,col] = size(theta1);
% 求Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    grad(j,1) = J2;
end
% 求 J，cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是矩阵版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J1 = sigmoid(X*theta);
% 求 J， cost function
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-logistic-regression&#34;&gt;Regularized logistic regression：&lt;/h2&gt;

&lt;p&gt;为什么要用到正则化呢？是因为有时候特征项我们选取的过多，有时候会出现过拟合问题。那么，一旦出现过拟合问题的时候，就会发生训练模型对训练集拟合非常好，但对新的数据就拟合偏差很大。这个时候，我们就需要用正则化项来惩罚参数，防止其过大，从而避免过拟合问题。同样，我一开始写的是迭代过程，后来又被我改成矩阵形式了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta1 = theta&#39;;
[row,col] = size(theta1);
% 求 Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    %对第一个参数 theta0 不进行正则化
    if j &amp;gt;= 2
        grad(j,1) = J2 + (lambda / m ) * theta(j);
    else
        grad(j,1) = J2;
    end
end
%求 J， cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
% 对第一个参数 theta0 不进行正则化
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;矩阵形式，和上面的logistics regression矩阵形式差不多，只需要注意不要对第一项进行处理即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
grad = grad + (lambda / m ) * theta;
grad(1) = grad(1) - (lambda / m ) * theta(1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对了，还有个predict函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : m
    XX = X(i:i,:);
    pp = sigmoid(XX*theta);
    if pp &amp;gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>利用PCL对三维点云进行面提取</title>
      <link>https://shiweihou.github.io/pointcloud/PCL_RANSAC/</link>
      <pubDate>Tue, 01 Nov 2016 21:15:04 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/PCL_RANSAC/</guid>
      <description>&lt;p&gt;因为要做三维重建，早上刚和老师讨论下基本流程，目前的想法是先提取出室内建筑物的线框结构，然后利用线框结构进行表面重建。后来有个师兄讲可以先利用PCL里RANSAC函数先进行面提取，所以就试了下。下面是主要代码，一些内容都写到注释里面去了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool LasFunction::ransac(void) {
    std::string fileName = &amp;quot;basementCut.las&amp;quot;;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud (new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    CLasOperator las;
    las.readLasFile(fileName.c_str());
    std::vector&amp;lt;Point3D&amp;gt; points = las.getPointData();
    for (size_t i = 0; i &amp;lt; points.size(); ++i) {
        pcl::PointXYZ onepoint;
        onepoint.x = points[i].x;
        onepoint.y = points[i].y;
        onepoint.z = points[i].z;
        cloud-&amp;gt;push_back(onepoint);
    }
    // Create the filtering object: downsample the dataset using a leaf size of 1cm
    // 用来进行降采样操作，应该就是用LeafSize进行，里面单位是以m为单位，在这么个立方体内，得到一个点
    pcl::VoxelGrid&amp;lt;pcl::PointXYZ&amp;gt; vg;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_filtered(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    vg.setInputCloud(cloud);
    vg.setLeafSize(0.1f, 0.1f, 0.1f);
    vg.filter(*cloud_filtered);
    std::cout &amp;lt;&amp;lt; &amp;quot;PointCloud before filtering has: &amp;quot; &amp;lt;&amp;lt; cloud-&amp;gt;points.size() &amp;lt;&amp;lt; &amp;quot; data points.&amp;quot; &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;PointCloud after filtering has: &amp;quot; &amp;lt;&amp;lt; cloud_filtered-&amp;gt;points.size() &amp;lt;&amp;lt; &amp;quot; data points.&amp;quot; &amp;lt;&amp;lt; std::endl;


    //构建一个ransac分割器，inliers存储索引，即到底哪些个点是在ransac得到的目标内，setModelType是用来表示ransac分割到底是分割成面或者分割成线或者分割成其它形状，coefficients用来存储系数，即加入表示的是个平面的话，点应该在类似于 a*x + b*y + c*z + d = 0内

    pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);
    pcl::PointIndices::Ptr inliers (new pcl::PointIndices);
    pcl::SACSegmentation&amp;lt;pcl::PointXYZ&amp;gt; sac;
    sac.setInputCloud(cloud);
    sac.setMethodType(pcl::SAC_RANSAC);
    sac.setModelType(pcl::SACMODEL_PLANE);
    //sac.setModelType(pcl::SACMODEL_LINE);
    sac.setDistanceThreshold(0.01);
    sac.setMaxIterations(100);
    sac.setProbability(0.9);


    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_plane(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;());
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_f(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;());
    int num = 1;

    // 保留原始点云中点云数量多少
    int nr_points = (int)cloud_filtered-&amp;gt;points.size();

    // 可以生成多个面，直到剩余点云中点的数量小于原始点云数量的 1/10
    while (cloud_filtered-&amp;gt;points.size() &amp;gt; 0.1 * nr_points) {
        sac.setInputCloud(cloud_filtered);
        sac.segment(*inliers, *coefficients);
        if (inliers-&amp;gt;indices.size() == 0) {
            std::cout &amp;lt;&amp;lt; &amp;quot;Could not estimate a planar nodel for the given dataset.&amp;quot; &amp;lt;&amp;lt; std::endl;
            break;
        }
        // 构建析取模型
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; extract;
        extract.setInputCloud(cloud_filtered);
        extract.setIndices(inliers);
        //如果为真，则前面放进去点除了索引里面的都会保留下来
        //否则，只保留下索引里面的点，通过filter函数将需要保留下的点放进去 
        extract.setNegative(false);     
        extract.filter(*cloud_plane);
        for (size_t i = 0; i &amp;lt; inliers-&amp;gt;indices.size(); ++i) {
            pcl::PointXYZ onePoint;
            onePoint.x = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].x;
            onePoint.y = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].y;
            onePoint.z = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].z;
            cloud_plane-&amp;gt;push_back(onePoint);
        }
        pcl::PCDWriter writer;
        std::string fileName = &amp;quot;ransacPlane&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        fileName += std::to_string(num);
        fileName += houzhui;
        writer.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName, *cloud_plane);
        std::cout &amp;lt;&amp;lt; fileName &amp;lt;&amp;lt; &amp;quot; is ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        ++num;

        //再设置为真，这样点云中除了索引里面的点，都会保留下来，这样我们就将前面提取出来的面里点去除掉了
        extract.setNegative(true);
        extract.filter(*cloud_f);
        cloud_filtered = cloud_f;
    }

    return true;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 分糖果</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-31-01/</link>
      <pubDate>Mon, 31 Oct 2016 20:01:12 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-31-01/</guid>
      <description>&lt;p&gt;题目链接：&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/candy/&#34;&gt;分糖果&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;一开始想的先排序，然后从低到高遍历，定义一个rat和pre，rat保存当前需要加的糖果数，pre为上一个rating，如果当前ratings[i] == pre， 就将rat置为1，不是就将rat++。。。。。后来发现不是这么做的，因为测试数据怎么都过不了。&lt;/p&gt;

&lt;p&gt;首先，不能排序！不能改变原有的顺序！题目的意思应该理解为“ 当前rating如果比其左右都高的话，其糖果个数不应该小于其左右的糖果个数”。 所以需要遍历两次，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先从左到右遍历一次，保证当右边的rating大于其左边的rating值时，糖果个数不小于左边的糖果值&lt;/li&gt;
&lt;li&gt;然后从右往左再遍历一次，保证当左边的rating大于其右边的rating值时，糖果个数不小于右边的糖果值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后所有的加起来就可以了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
    /**
     * @param ratings Children&#39;s ratings
     * @return the minimum candies you must give
     */
    int candy(vector&amp;lt;int&amp;gt;&amp;amp; ratings) {
        // Write your code here
        int sum = 0;
        int len = ratings.size();
        int *num = new int[len];

        num[0] = 1;
        // 从左往右
        for (int i = 1; i &amp;lt; len; ++i) {
            num[i] = ratings[i] &amp;gt; ratings[i-1] ? num[i-1] + 1 : 1;
        }
        //从右往左
        for (int i = len - 1; i &amp;gt; 0; --i) {
            num[i - 1] = ratings[i] &amp;lt; ratings[i-1] ? max(num[i] + 1, num[i - 1]) : num[i - 1];
        }
        for (int i = 0; i &amp;lt; len; ++i) sum += num[i];

        delete []num;
        return sum;
    }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;，&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pcl 1.8.0 ALL in one &#43; vs 2013</title>
      <link>https://shiweihou.github.io/pointcloud/pcl/</link>
      <pubDate>Mon, 31 Oct 2016 15:45:15 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl/</guid>
      <description>&lt;p&gt;最近需要做三维重建，要用到pcl函数库里带的 RegionGrowing 函数，官网上只有1.6.0的老版本安装包，而且貌似1.6.0版本里没有集成完全的 RegionGrowing 函数，遂找到1.8.0版本的PCL，进行处理。&lt;/p&gt;

&lt;p&gt;网上目前有很多根据源码利用Cmake来编译PCL的教程，后来在一个&lt;a href=&#34;http://unanancyowen.com/?p=2009&amp;amp;lang=en&#34;&gt;日本人的博客&lt;/a&gt;上找到PCL 1.8.0的 all-in-one，遂下载安装之。&lt;/p&gt;

&lt;p&gt;我的电脑是win10 64 + vs 2013， 于是下载的是PCL 1.8.0 All-in-one Installer MSVC2013 Win64 这个版本。&lt;/p&gt;

&lt;p&gt;安装过程：&lt;/p&gt;

&lt;p&gt;下载安装：下载并安装pcl_all_in_one 1.8.0,安装时选择“Add PCL to the system PATH for all users&amp;rdquo;选项，选择自己的安装路径，我是默认路径 C:\Program Files\PCL 1.8.0，然后一直点击下一步即可。注意在安装过程中会提示你安装OPENNI库，不要选择默认路径，要放在PCL路径下的3rdParty文件夹下，例如我的就安装在了 C:\Program Files\PCL 1.8.0\3rdParty\OpenNI2 这个文件夹内。&lt;/p&gt;

&lt;p&gt;配置环境变量：&lt;/p&gt;

&lt;p&gt;如果在上面的安装步骤中选择了“Add PCL to the system PATH for all users&amp;rdquo;选项，那么系统环境变量中会出现下图四个路径：如果没有的话可能就需要你手动添加了（一般出现这种情况的时候安装过程中就会提示“因为路径名太长无法写进去”之类的话） &lt;img src=&#34;https://raw.githubusercontent.com/shiweiHou/image/master/PCL-Install-image1.png&#34; alt=&#34;image1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在系统变量Path中加入：&lt;img src=&#34;https://raw.githubusercontent.com/shiweiHou/image/master/PCL-Install-image2.png&#34; alt=&#34;image2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设置包含目录：&lt;/p&gt;

&lt;p&gt;打开vs2013，新建win32控制台项目，选择X64平台，如果没有就新建个，然后在新建项目的属性管理器的中的VC++的包含目录中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Program Files\PCL 1.8.0\3rdParty\OpenNI2\Include\Win32;
C:\Program Files\PCL 1.8.0\3rdParty\Eigen\eigen3;
C:\Program Files\PCL 1.8.0\3rdParty\VTK\include\vtk-7.0;
C:\Program Files\PCL 1.8.0\3rdParty\FLANN\include\;
C:\Program Files\PCL 1.8.0\3rdParty\Qhull\include;
C:\Program Files\PCL 1.8.0\3rdParty\Boost\include\boost-1_61;
C:\Program Files\PCL 1.8.0\include\pcl-1.8;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在VC++的库目录中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c:\Program Files\PCL 1.8.0\lib;
c:\Program Files\PCL 1.8.0\3rdParty\Qhull\lib;
c:\Program Files\PCL 1.8.0\3rdParty\FLANN\lib;
c:\Program Files\PCL 1.8.0\3rdParty\Boost\lib;
c:\Program Files\PCL 1.8.0\3rdParty\VTK\lib;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置附加依赖项：
DEBUG模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcl_common_debug.lib
pcl_features_debug.lib
pcl_filters_debug.lib
pcl_io_debug.lib
pcl_io_ply_debug.lib
pcl_kdtree_debug.lib
pcl_keypoints_debug.lib
pcl_octree_debug.lib
pcl_outofcore_debug.lib
pcl_people_debug.lib
pcl_recognition_debug.lib
pcl_registration_debug.lib
pcl_sample_consensus_debug.lib
pcl_search_debug.lib
pcl_segmentation_debug.lib
pcl_surface_debug.lib
pcl_tracking_debug.lib
pcl_visualization_debug.lib
libboost_atomic-vc120-mt-gd-1_61.lib
libboost_chrono-vc120-mt-gd-1_61.lib
libboost_container-vc120-mt-gd-1_61.lib
libboost_context-vc120-mt-gd-1_61.lib
libboost_coroutine-vc120-mt-gd-1_61.lib
libboost_date_time-vc120-mt-gd-1_61.lib
libboost_exception-vc120-mt-gd-1_61.lib
libboost_filesystem-vc120-mt-gd-1_61.lib
libboost_graph-vc120-mt-gd-1_61.lib
libboost_iostreams-vc120-mt-gd-1_61.lib
libboost_locale-vc120-mt-gd-1_61.lib
libboost_log-vc120-mt-gd-1_61.lib
libboost_log_setup-vc120-mt-gd-1_61.lib
libboost_math_c99-vc120-mt-gd-1_61.lib
libboost_math_c99f-vc120-mt-gd-1_61.lib
libboost_math_c99l-vc120-mt-gd-1_61.lib
libboost_math_tr1-vc120-mt-gd-1_61.lib
libboost_math_tr1f-vc120-mt-gd-1_61.lib
libboost_math_tr1l-vc120-mt-gd-1_61.lib
libboost_mpi-vc120-mt-gd-1_61.lib
libboost_prg_exec_monitor-vc120-mt-gd-1_61.lib
libboost_program_options-vc120-mt-gd-1_61.lib
libboost_random-vc120-mt-gd-1_61.lib
libboost_regex-vc120-mt-gd-1_61.lib
libboost_serialization-vc120-mt-gd-1_61.lib
libboost_signals-vc120-mt-gd-1_61.lib
libboost_system-vc120-mt-gd-1_61.lib
libboost_test_exec_monitor-vc120-mt-gd-1_61.lib
libboost_thread-vc120-mt-gd-1_61.lib
libboost_timer-vc120-mt-gd-1_61.lib
libboost_type_erasure-vc120-mt-gd-1_61.lib
libboost_unit_test_framework-vc120-mt-gd-1_61.lib
libboost_wave-vc120-mt-gd-1_61.lib
libboost_wserialization-vc120-mt-gd-1_61.lib
flann_cpp_s-gd.lib
flann_s-gd.lib
flann-gd.lib
qhull_d.lib
qhull_p_d.lib
qhull_r_d.lib
qhullcpp_d.lib
qhullstatic_d.lib
qhullstatic_r_d.lib
vtkChartsCore-7.0-gd.lib
vtkCommonColor-7.0-gd.lib
vtkCommonComputationalGeometry-7.0-gd.lib
vtkCommonCore-7.0-gd.lib
vtkCommonDataModel-7.0-gd.lib
vtkCommonExecutionModel-7.0-gd.lib
vtkCommonMath-7.0-gd.lib
vtkCommonMisc-7.0-gd.lib
vtkCommonSystem-7.0-gd.lib
vtkCommonTransforms-7.0-gd.lib
vtkDICOMParser-7.0-gd.lib
vtkDomainsChemistry-7.0-gd.lib
vtkFiltersAMR-7.0-gd.lib
vtkFiltersCore-7.0-gd.lib
vtkFiltersExtraction-7.0-gd.lib
vtkFiltersFlowPaths-7.0-gd.lib
vtkFiltersGeneral-7.0-gd.lib
vtkFiltersGeneric-7.0-gd.lib
vtkFiltersGeometry-7.0-gd.lib
vtkFiltersHybrid-7.0-gd.lib
vtkFiltersHyperTree-7.0-gd.lib
vtkFiltersImaging-7.0-gd.lib
vtkFiltersModeling-7.0-gd.lib
vtkFiltersParallel-7.0-gd.lib
vtkFiltersParallelImaging-7.0-gd.lib
vtkFiltersProgrammable-7.0-gd.lib
vtkFiltersSMP-7.0-gd.lib
vtkFiltersSelection-7.0-gd.lib
vtkFiltersSources-7.0-gd.lib
vtkFiltersStatistics-7.0-gd.lib
vtkFiltersTexture-7.0-gd.lib
vtkFiltersVerdict-7.0-gd.lib
vtkGeovisCore-7.0-gd.lib
vtkIOAMR-7.0-gd.lib
vtkIOCore-7.0-gd.lib
vtkIOEnSight-7.0-gd.lib
vtkIOExodus-7.0-gd.lib
vtkIOExport-7.0-gd.lib
vtkIOGeometry-7.0-gd.lib
vtkIOImage-7.0-gd.lib
vtkIOImport-7.0-gd.lib
vtkIOInfovis-7.0-gd.lib
vtkIOLSDyna-7.0-gd.lib
vtkIOLegacy-7.0-gd.lib
vtkIOMINC-7.0-gd.lib
vtkIOMovie-7.0-gd.lib
vtkIONetCDF-7.0-gd.lib
vtkIOPLY-7.0-gd.lib
vtkIOParallel-7.0-gd.lib
vtkIOParallelXML-7.0-gd.lib
vtkIOSQL-7.0-gd.lib
vtkIOVideo-7.0-gd.lib
vtkIOXML-7.0-gd.lib
vtkIOXMLParser-7.0-gd.lib
vtkImagingColor-7.0-gd.lib
vtkImagingCore-7.0-gd.lib
vtkImagingFourier-7.0-gd.lib
vtkImagingGeneral-7.0-gd.lib
vtkImagingHybrid-7.0-gd.lib
vtkImagingMath-7.0-gd.lib
vtkImagingMorphological-7.0-gd.lib
vtkImagingSources-7.0-gd.lib
vtkImagingStatistics-7.0-gd.lib
vtkImagingStencil-7.0-gd.lib
vtkInfovisCore-7.0-gd.lib
vtkInfovisLayout-7.0-gd.lib
vtkInteractionImage-7.0-gd.lib
vtkInteractionStyle-7.0-gd.lib
vtkInteractionWidgets-7.0-gd.lib
vtkNetCDF-7.0-gd.lib
vtkNetCDF_cxx-7.0-gd.lib
vtkParallelCore-7.0-gd.lib
vtkRenderingAnnotation-7.0-gd.lib
vtkRenderingContext2D-7.0-gd.lib
vtkRenderingContextOpenGL-7.0-gd.lib
vtkRenderingCore-7.0-gd.lib
vtkRenderingFreeType-7.0-gd.lib
vtkRenderingGL2PS-7.0-gd.lib
vtkRenderingImage-7.0-gd.lib
vtkRenderingLIC-7.0-gd.lib
vtkRenderingLOD-7.0-gd.lib
vtkRenderingLabel-7.0-gd.lib
vtkRenderingOpenGL-7.0-gd.lib
vtkRenderingVolume-7.0-gd.lib
vtkRenderingVolumeOpenGL-7.0-gd.lib
vtkViewsContext2D-7.0-gd.lib
vtkViewsCore-7.0-gd.lib
vtkViewsInfovis-7.0-gd.lib
vtkalglib-7.0-gd.lib
vtkexoIIc-7.0-gd.lib
vtkexpat-7.0-gd.lib
vtkfreetype-7.0-gd.lib
vtkgl2ps-7.0-gd.lib
vtkhdf5-7.0-gd.lib
vtkhdf5_hl-7.0-gd.lib
vtkjpeg-7.0-gd.lib
vtkjsoncpp-7.0-gd.lib
vtklibxml2-7.0-gd.lib
vtkmetaio-7.0-gd.lib
vtkoggtheora-7.0-gd.lib
vtkpng-7.0-gd.lib
vtkproj4-7.0-gd.lib
vtksqlite-7.0-gd.lib
vtksys-7.0-gd.lib
vtktiff-7.0-gd.lib
vtkverdict-7.0-gd.lib
vtkzlib-7.0-gd.lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RELEASE模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcl_common_release.lib
pcl_features_release.lib
pcl_filters_release.lib
pcl_io_ply_release.lib
pcl_io_release.lib
pcl_kdtree_release.lib
pcl_keypoints_release.lib
pcl_octree_release.lib
pcl_outofcore_release.lib
pcl_people_release.lib
pcl_recognition_release.lib
pcl_registration_release.lib
pcl_sample_consensus_release.lib
pcl_search_release.lib
pcl_segmentation_release.lib
pcl_surface_release.lib
pcl_tracking_release.lib
pcl_visualization_release.lib
libboost_atomic-vc120-mt-1_61.lib
libboost_chrono-vc120-mt-1_61.lib
libboost_container-vc120-mt-1_61.lib
libboost_context-vc120-mt-1_61.lib
libboost_coroutine-vc120-mt-1_61.lib
libboost_date_time-vc120-mt-1_61.lib
libboost_exception-vc120-mt-1_61.lib
libboost_filesystem-vc120-mt-1_61.lib
libboost_graph-vc120-mt-1_61.lib
libboost_iostreams-vc120-mt-1_61.lib
libboost_locale-vc120-mt-1_61.lib
libboost_log-vc120-mt-1_61.lib
libboost_log_setup-vc120-mt-1_61.lib
libboost_math_c99-vc120-mt-1_61.lib
libboost_math_c99f-vc120-mt-1_61.lib
libboost_math_c99l-vc120-mt-1_61.lib
libboost_math_tr1-vc120-mt-1_61.lib
libboost_math_tr1f-vc120-mt-1_61.lib
libboost_math_tr1l-vc120-mt-1_61.lib
libboost_mpi-vc120-mt-1_61.lib
libboost_prg_exec_monitor-vc120-mt-1_61.lib
libboost_program_options-vc120-mt-1_61.lib
libboost_random-vc120-mt-1_61.lib
libboost_regex-vc120-mt-1_61.lib
libboost_serialization-vc120-mt-1_61.lib
libboost_signals-vc120-mt-1_61.lib
libboost_system-vc120-mt-1_61.lib
libboost_test_exec_monitor-vc120-mt-1_61.lib
libboost_thread-vc120-mt-1_61.lib
libboost_timer-vc120-mt-1_61.lib
libboost_type_erasure-vc120-mt-1_61.lib
libboost_unit_test_framework-vc120-mt-1_61.lib
libboost_wave-vc120-mt-1_61.lib
libboost_wserialization-vc120-mt-1_61.lib
flann_cpp_s.lib
flann_s.lib
flann.lib
qhull.lib
qhull_p.lib
qhull_r.lib
qhullcpp.lib
qhullstatic.lib
qhullstatic_r.lib
vtkChartsCore-7.0.lib
vtkCommonColor-7.0.lib
vtkCommonComputationalGeometry-7.0.lib
vtkCommonCore-7.0.lib
vtkCommonDataModel-7.0.lib
vtkCommonExecutionModel-7.0.lib
vtkCommonMath-7.0.lib
vtkCommonMisc-7.0.lib
vtkCommonSystem-7.0.lib
vtkCommonTransforms-7.0.lib
vtkDICOMParser-7.0.lib
vtkDomainsChemistry-7.0.lib
vtkFiltersAMR-7.0.lib
vtkFiltersCore-7.0.lib
vtkFiltersExtraction-7.0.lib
vtkFiltersFlowPaths-7.0.lib
vtkFiltersGeneral-7.0.lib
vtkFiltersGeneric-7.0.lib
vtkFiltersGeometry-7.0.lib
vtkFiltersHybrid-7.0.lib
vtkFiltersHyperTree-7.0.lib
vtkFiltersImaging-7.0.lib
vtkFiltersModeling-7.0.lib
vtkFiltersParallel-7.0.lib
vtkFiltersParallelImaging-7.0.lib
vtkFiltersProgrammable-7.0.lib
vtkFiltersSMP-7.0.lib
vtkFiltersSelection-7.0.lib
vtkFiltersSources-7.0.lib
vtkFiltersStatistics-7.0.lib
vtkFiltersTexture-7.0.lib
vtkFiltersVerdict-7.0.lib
vtkGeovisCore-7.0.lib
vtkIOAMR-7.0.lib
vtkIOCore-7.0.lib
vtkIOEnSight-7.0.lib
vtkIOExodus-7.0.lib
vtkIOExport-7.0.lib
vtkIOGeometry-7.0.lib
vtkIOImage-7.0.lib
vtkIOImport-7.0.lib
vtkIOInfovis-7.0.lib
vtkIOLSDyna-7.0.lib
vtkIOLegacy-7.0.lib
vtkIOMINC-7.0.lib
vtkIOMovie-7.0.lib
vtkIONetCDF-7.0.lib
vtkIOPLY-7.0.lib
vtkIOParallel-7.0.lib
vtkIOParallelXML-7.0.lib
vtkIOSQL-7.0.lib
vtkIOVideo-7.0.lib
vtkIOXML-7.0.lib
vtkIOXMLParser-7.0.lib
vtkImagingColor-7.0.lib
vtkImagingCore-7.0.lib
vtkImagingFourier-7.0.lib
vtkImagingGeneral-7.0.lib
vtkImagingHybrid-7.0.lib
vtkImagingMath-7.0.lib
vtkImagingMorphological-7.0.lib
vtkImagingSources-7.0.lib
vtkImagingStatistics-7.0.lib
vtkImagingStencil-7.0.lib
vtkInfovisCore-7.0.lib
vtkInfovisLayout-7.0.lib
vtkInteractionImage-7.0.lib
vtkInteractionStyle-7.0.lib
vtkInteractionWidgets-7.0.lib
vtkNetCDF-7.0.lib
vtkNetCDF_cxx-7.0.lib
vtkParallelCore-7.0.lib
vtkRenderingAnnotation-7.0.lib
vtkRenderingContext2D-7.0.lib
vtkRenderingContextOpenGL-7.0.lib
vtkRenderingCore-7.0.lib
vtkRenderingFreeType-7.0.lib
vtkRenderingGL2PS-7.0.lib
vtkRenderingImage-7.0.lib
vtkRenderingLIC-7.0.lib
vtkRenderingLOD-7.0.lib
vtkRenderingLabel-7.0.lib
vtkRenderingOpenGL-7.0.lib
vtkRenderingVolume-7.0.lib
vtkRenderingVolumeOpenGL-7.0.lib
vtkViewsContext2D-7.0.lib
vtkViewsCore-7.0.lib
vtkViewsInfovis-7.0.lib
vtkalglib-7.0.lib
vtkexoIIc-7.0.lib
vtkexpat-7.0.lib
vtkfreetype-7.0.lib
vtkgl2ps-7.0.lib
vtkhdf5-7.0.lib
vtkhdf5_hl-7.0.lib
vtkjpeg-7.0.lib
vtkjsoncpp-7.0.lib
vtklibxml2-7.0.lib
vtkmetaio-7.0.lib
vtkoggtheora-7.0.lib
vtkpng-7.0.lib
vtkproj4-7.0.lib
vtksqlite-7.0.lib
vtksys-7.0.lib
vtktiff-7.0.lib
vtkverdict-7.0.lib
vtkzlib-7.0.lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意事项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在安装pcl 1.8.0 all-in-one和配置好环境变量之后，需重启下计算机，再进入vs配置&lt;/li&gt;
&lt;li&gt;注意OPENNI的安装路径不要搞错了&lt;/li&gt;
&lt;li&gt;PCL 1.8.0在vs中会有个问题，就是运行的时候会提示：&lt;code&gt;error C4996: &#39;pcl::SAC_SAMPLE_SIZE&#39;: This map is deprecated and is kept only to prevent breaking existing user code. Starting from PCL 1.8.0 model sample size is a protected member of the SampleConsensusModel class&lt;/code&gt;，这个是1.8.0的小毛病，解决方法是：打开项目属性页 -&amp;gt; C/C++ -&amp;gt; 常规 -&amp;gt; SDL检查(设置为否)。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>线性回归</title>
      <link>https://shiweihou.github.io/machinelearning/ex1/</link>
      <pubDate>Sun, 30 Oct 2016 16:10:31 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex1/</guid>
      <description>

&lt;p&gt;从今天开始，开始记录学习机器学习学习过程。目前的学习方法是根据&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Coursera&lt;/a&gt;上Andrew Ng大牛的机器学习课程，据说是同名的斯坦福大学公开课的简化版本，仅介绍基本原理及提供配套的练习，非常适合新手入门。强烈推荐大家去看下视频，讲解的很详细，而且还有配套的演示过程，非常适合新手。&lt;/p&gt;

&lt;p&gt;有关线性回归的具体内容我就不多说了，网上例子有很多，教程有很多，我只谈下在学习过程中自己遇到的一些问题及看法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在学习过程中，参数需要同时更新，不能用更新好的参数去更新下一个参数，例如不能用更新好的 theta0 去更新 theta1.&lt;/li&gt;
&lt;li&gt;如果数据特征变化尺度过大，例如x1在[1,10]范围内，而x2就在[10000,10000000]范围内，就需要进行特征缩放，重新缩放特征的范围到[0, 1]或[-1, 1]。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;顺便贴下第一课的课后作业的答案：&lt;/p&gt;

&lt;h2 id=&#34;computecost-m&#34;&gt;computeCost.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m,
J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computecostmulti-m&#34;&gt;computeCostMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCostMulti(X, y, theta)
%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables
%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m 
    JJ = 0;
    for j = 1 : size(theta,1)
        JJ = JJ + theta(j) * X(i,j);
    end
    JJ = JJ - y(i);
    J = J + JJ ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;featurenormalize-m&#34;&gt;featureNormalize.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.
% You need to set these values correctly
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
% ====================== YOUR CODE HERE ======================
% Instructions: First, for each feature dimension, compute the mean
%               of the feature and subtract it from the dataset,
%               storing the mean value in mu. Next, compute the 
%               standard deviation of each feature and divide
%               each feature by it&#39;s standard deviation, storing
%               the standard deviation in sigma. 
%
%               Note that X is a matrix where each column is a 
%               feature and each row is an example. You need 
%               to perform the normalization separately for 
%               each feature. 
%
% Hint: You might find the &#39;mean&#39; and &#39;std&#39; functions useful.
%       
avg = mean(X);
piancha = std(X);
for row = 1 : size(X,1)
    for col = 1 : size(X,2)
        X_norm(row,col) = (X(row,col) - avg(col)) / piancha(col);
    end
end
mu = avg;
sigma = piancha;
% ============================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescent-m&#34;&gt;gradientDescent.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescentmulti-m&#34;&gt;gradientDescentMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end

end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 二进制表示</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-13-02/</link>
      <pubDate>Thu, 13 Oct 2016 20:46:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-13-02/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/binary-representation/&#34;&gt;题目链接&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;说是困难题，其实很简单的，就是将字符串中整数部分和小数部分分别提取出来，然后根据整数化二进制和小数划二进制的形式来求。唯一有问题的就是字符串中的小数部分化成小数时，如果按照传统的一个个来做的话，在计算机中表示很奇怪，例如对于&lt;code&gt;4096.6435546875&lt;/code&gt;，小数部分按照6*10（-1） + 4*10（-2）&amp;hellip;这样求的话，在计算机中实际表示为&lt;code&gt;0.64355468750000011&lt;/code&gt;，所以很奇怪，估计是和计算机中浮点型的存储方式有关，所以后来就直接用C++自带的库函数&lt;code&gt;atof()&lt;/code&gt;函数来转换，这样就ok了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
/**
 *@param n: Given a decimal number that is passed in as a string
 *@return: A string
 */
string binaryRepresentation(string n) {
    // wirte your code here
    string ret;
    if (n.empty()) return 0;
    int len = n.size();
    int i = 0;
    while (i &amp;lt; len &amp;amp;&amp;amp; n[i] != &#39;.&#39;) ++i;
    int j = 0;
    bool flag = true;
    if (n[0] == &#39;-&#39;) {
        flag = false;
        ++j;
        ret += &amp;quot;1&amp;quot;;
    }

    long number = 0;
    while (j &amp;lt; i) {
        number = number * 10 + (n[j] - &#39;0&#39;);
        ++j;
    }
    longToBinary(number, ret);

    if (i == len) return ret;
    ret += &amp;quot;.&amp;quot;;


    double number1 = 0.0;
    ++i;
    int count = -1;
    while (i &amp;lt; len) {
        number1 += (n[i] - &#39;0&#39;) * (double)pow(10, count);
        ++i;
        --count;
    }

    bool yes = true;
    // 自己计算的不行，还得靠库函数
    double decPart = atof(n.substr(n.find(&amp;quot;.&amp;quot;), n.size() - n.find(&amp;quot;.&amp;quot;)).c_str());
    number1 = decPart;
    yes = digitToBinary(number1, ret);

    int len1 = ret.size();
    if (ret[len1-1] == &#39;.&#39;) ret = ret.substr(0,len1-1);
    if (yes) return ret;
    ret = &amp;quot;ERROR&amp;quot;;
    return ret;
}

bool digitToBinary(double number, string &amp;amp;s) {
    int time = 0;
    double number1 = 1.0;
    double number2 = 0.0;
    while (time &amp;lt;= 32) {
        if (number == number1) 
            break;
        if (number == number2) 
            break;
        number *= 2;
        if (number &amp;gt;= 1) {
            number -= 1.0;
            s += to_string(1);
        } else s += to_string(0);

        ++time;
    }

    if (time &amp;lt;= 32) return true;
    return false;
}
void longToBinary(long number, string &amp;amp;s) {
    vector&amp;lt;int&amp;gt; vb;
    while (number != 0) {
        vb.push_back(number % 2) ;
        number /= 2;
    }
    if (vb.empty()) {
        s += to_string(0);
        return;
    }
    reverse(vb.begin(), vb.end());
    for (auto e : vb) {
        s += to_string(e);
    }

    return ;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;};&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> LintCode 逆波兰表达式求值</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-13-01/</link>
      <pubDate>Thu, 13 Oct 2016 20:43:48 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-13-01/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/evaluate-reverse-polish-notation/&#34;&gt;题目链接&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;题意很明确，其实就是四则运算的后缀求值，用栈来维护，中间过程中没有考虑栈元素不够或者除数为0的情况，但也AC了，说明输入的数据比较严格。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int evalRPN(vector&amp;lt;string&amp;gt;&amp;amp; tokens) {
    // Write your code here
    stack&amp;lt;int&amp;gt; s;
    int ans;
    if (tokens.empty()) return ans;
    for (auto e : tokens) {
        if (e != &amp;quot;+&amp;quot; &amp;amp;&amp;amp; e != &amp;quot;-&amp;quot; &amp;amp;&amp;amp; e != &amp;quot;*&amp;quot; &amp;amp;&amp;amp; e != &amp;quot;/&amp;quot;) {
            int number = stoi(e);
            s.push(number);
            continue;
        } 
        int x = s.top();
        s.pop();
        int y = s.top();
        s.pop();
        if (e == &amp;quot;+&amp;quot;) s.push(x + y);
        if (e == &amp;quot;-&amp;quot;) s.push(y - x);
        if (e == &amp;quot;/&amp;quot;) s.push(y / x);
        if (e == &amp;quot;*&amp;quot;) s.push(x * y);
    }

    return s.top();
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 买卖股票的最佳时机（1 2 3 4）</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-11-01/</link>
      <pubDate>Tue, 11 Oct 2016 20:21:15 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-11-01/</guid>
      <description>

&lt;h2 id=&#34;买卖股票的最佳时机-1-1&#34;&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/best-time-to-buy-and-sell-stock/&#34;&gt;买卖股票的最佳时机&lt;/a&gt; 1&lt;/h2&gt;

&lt;p&gt;题目就不贴了，链接在上面&lt;/p&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;题意很简单，因为我们最多只可以完成一次交易，而且股票交易必须在前面买入才可以卖出，所以我们只需要从前往后遍历，维护两个变量值，minPrice表示所有的股票中价钱最低的那个，maxProfit表示到第i天所能获得的最大利润。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    if (prices.empty()) return 0;

    int n = prices.size();Lint
    int minPrice = prices[0];
    int maxProfit = 0;

    for (int i = 1; i &amp;lt; n; ++i) {
    // 如果当前股票价格还要小于前面的已知的最低价格，就替换之，当然不需要进行卖出，相反还要买入
        if (minPrice &amp;gt; prices[i]) {
            minPrice = prices[i];
        } else  // 否则的话，在第i天使可以尝试卖出的，更新maxProfit
            maxProfit = max(maxProfit, prices[i] - minPrice);
    }

    return maxProfit;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;买卖股票的最佳时机-ii-2&#34;&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/best-time-to-buy-and-sell-stock-ii/&#34;&gt;买卖股票的最佳时机 II&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;最笨的一种方法。因为要尽可能的获得最大利润，当然是完成每次交易的获得的利润越大越好。因为我没有想到当天卖出再买入这件事，只是想到一天只能买入或者卖出，所以得到如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    int sum = 0;
    if (prices.empty() || prices.size() &amp;lt; 2) return sum;

    int n = prices.size();

    int i = 1;
    // buy 记录买入股票的价钱
    int buy = prices[0];
    while ( i &amp;lt; n ) {
    // 如果当天的价格还在上涨，那就不卖出，继续往下看；如果不是，说明从buy的那一天到i-1天都是上涨的，那么就在第i-1天卖出，继续往下看。
        if (prices[i] &amp;gt; prices[i-1]) {
            ++i;
            if (i == n) sum += prices[i-1] - buy;
        }
        else {
            sum += prices[i-1] - buy;
            buy = prices[i];
            ++i;
        }
    }

    return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网上还有一种更简单的算法，就是尽可能交易更多的次数，当天可以买也可以卖，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    int sum = 0;
    if (prices.empty() || prices.size() &amp;lt; 2) return sum;

    int n = prices.size();
    for (int i = 1; i &amp;lt; n; ++i) {
    // 意思是只要当天卖可以有利润，就卖出去，再买回来，继续往后看
        if (prices[i] &amp;gt; prices[i-1])
            sum += prices[i] - prices[i-1];
    }

    return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;买卖股票的最佳时机-iii-3&#34;&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/best-time-to-buy-and-sell-stock-iii/&#34;&gt;买卖股票的最佳时机 III&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;因为是可以完成两笔交易，所以最最容易想到的办法就是对每个时间点i，分别计算1~i 和 i~n 所能获得的最大利润和。但这样时间复杂度为 O(n^2)。但是我们知道我们在计算1~i的最大利润的时候，1~i-1的利润已经计算过了，不需要再重新计算一遍，因此：&lt;strong&gt;保存两个数组，left[i], right[i],分别记录从左到右和从右往左分别到i位置所能产生的最大利润，最后直接遍历一遍两个数组就可以了，找出最大的left[i] + right[i] 组合，该解法实际上就是将 1 的解法双向各执行一遍记录结果&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    if(prices.empty()) return 0;

    vector&amp;lt;int&amp;gt; left (prices.size(), 0);
    vector&amp;lt;int&amp;gt; right (prices.size(), 0);

    int leftMin = prices[0];
    int rightMax = prices[prices.size()-1];

    int sum = 0;
    //计算左半段最大收益
    for(int i = 1 ; i &amp;lt; prices.size(); ++i){
        leftMin = min(prices[i], leftMin);
        left[i] = max(prices[i] - leftMin, left[i-1]);
    }
    //计算右半段最大收益
    for(int i = prices.size() - 2 ; i &amp;gt;= 0; --i){
        rightMax = max(prices[i], rightMax);
        right[i] = max(rightMax - prices[i], right[i+1]);
    }
    //找出两次交易最大收益组合
    for(int i = 0 ; i &amp;lt; prices.size(); ++i){
        if( (left[i]+right[i]) &amp;gt; sum ) 
            sum = left[i] + right[i];
    }
    return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在网上还看到一种解法，十分巧妙：其实我们并不需要知道每个时间点买卖第一第二笔股票收益的全部信息，我们只要知道前一个时间点买卖第一第二笔股票的最大收益信息，就可以直到当前最大的收益信息了，这样可以为我们省去额外空间。这里我们遍历prices数组的时候，维护四个变量:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;release2是在该价格点卖出第二笔股票后手里剩的钱，等于上一轮买入第二笔股票后手里剩的钱加上卖出当前股票价格的钱，或者上一轮卖出第二笔股票后手里剩的钱两者中较大的。&lt;/li&gt;
&lt;li&gt;hold2是在该价格点买入第二笔股票后手里剩的钱，等于上一轮卖出第一笔股票后手里剩的钱减去买入当前股票价格的钱，或者上一轮买入第二笔股票后手里剩的钱两者中较大的。&lt;/li&gt;
&lt;li&gt;release1是在该价格点卖出第一笔股票后手里剩的钱，等于上一轮买入第一笔股票后手里剩的钱加上卖出当前股票价格的钱，或者上一轮卖出第一笔股票后手里剩的钱两者中较大的。&lt;/li&gt;
&lt;li&gt;hold1是在该价格点买入第一笔股票后手里剩的钱，等于初始资金减去买入当前股票价格的钱或者初始资金（不买）中较大的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里计算顺序按照release2 -&amp;gt; hold2 -&amp;gt; release1 -&amp;gt; hold1，因为卖是要后于买的，而第二次交易也是后于第一次交易的，通过这个顺序我们能用这些变量自身来记录上次的值。相当于release2的时间点要先于hold1四个点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    int hold1 = INT_MIN, hold2 = INT_MIN;
    int release1 = 0, release2 = 0;
    int n = prices.size();
    for(int i = 0; i &amp;lt; n; ++i){
        // 在该价格点卖出第二笔股票后手里剩的钱，等于上一轮买入第二笔股票后手里剩的钱加上卖出当前股票价格的钱
        // 或者上一轮卖出第二笔股票后手里剩的钱两者中较大的
        release2 = max(release2, hold2 + prices[i]);
        // 在该价格点买入第二笔股票后手里剩的钱，等于上一轮卖出第一笔股票后手里剩的钱减去买入当前股票价格的钱
        // 或者上一轮买入第二笔股票后手里剩的钱两者中较大的
        hold2 = max(hold2, release1 - prices[i]);
        // 在该价格点卖出第一笔股票后手里剩的钱，等于上一轮买入第一笔股票后手里剩的钱加上卖出当前股票价格的钱
        // 或者上一轮卖出第一笔股票后手里剩的钱两者中较大的
        release1 = max(release1, hold1 + prices[i]);
        // 在该价格点买入第一笔股票后手里剩的钱，等于初始资金减去买入当前股票价格的钱
        // 或者初始资金（不买）中较大的
        hold1 = max(hold1, 0 - prices[i]);
    }
    return release2;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;买卖股票的最佳时机-iv-4&#34;&gt;&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/best-time-to-buy-and-sell-stock-iv/&#34;&gt;买卖股票的最佳时机 IV&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;我们将第 i 天已经执行 j 笔交易的最大收益作为全局变量 global[i][j], 将第 i 天正好完成第 j 笔交易的最大收益作为局部变量 local[i][j].&lt;/p&gt;

&lt;p&gt;也就是说， global[i][j] 就是我们最终要得到的结果，但 local[i][j] 的意思是，再第i天必须当天完成第j笔交易，而 global[i][j] 则不需要。&lt;/p&gt;

&lt;p&gt;对于 global[i][j], 也就是我们要知道的第i天已经完成j笔交易获得的最大收益，可以基于第 i-1 天完成 j 笔交易的最大收益与 第 i 天正好完成第 j 笔交易的最大收益，即 global[i][j] = max ( global[i-1][j], local[i][j] ).&lt;/p&gt;

&lt;p&gt;对于 local[i][j], 也就是我们要求的第i天刚好完成第j笔交易的最大收益，可以基于第i-1天正好完成第j-1笔交易的最大收益加上当天交易的差值， 还有第i-1天正好完成第j笔交易的最大收益加上当天交易的差值。 要注意的是，&lt;/p&gt;

&lt;p&gt;第i-1天正好完成第j-1笔交易的这种情况，当前交易的差值去0和实际昨天今天差价中较大的那个，因为如果我们还剩下1次交易机会，如果prices[i] &amp;gt; prices[i-1],我们完全可以在第i-1天完成第j-1笔交易后，再当天买入，第i天卖出。&lt;/p&gt;

&lt;p&gt;但是对于第i-1天正好完成第j笔交易这种情况，因为第i-1天正好完成第j笔交易，那么第i天的交易其实在第i-1天交易里面，也就是第i天要连着第i-1天交易，使得第i-1天完成的第j笔交易正好和第i天完成的第j笔交易是同一天交易，所以无论prices[i]是否大于prices[i-1], 这次交易都要进行下去，所以local[i][j] = max ( global[i-1][j-1] + max (diff, 0), local[i-1][j] + diff);diff = prices[i] - prices[i-1].&lt;/p&gt;

&lt;p&gt;PS:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于k &amp;gt; n /
2的情况，我们可以用II的解法来节省空间。因为按照题意必须先买后卖，那么对于n天交易，能够产生有效收益的交易次数是小于等于n/2的，只有不同天买卖才能产生差价。对于大于n/2的那部分交易，必定是当天买卖没有任何收益的，无论交易多少次都是一样的。所以如果k &amp;gt; n / 2，就相当于无限次交易。
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种方法理解了很简单，但真要自己写出来很难。我也没有写出来，还是参考的别人的答案才做出来，还需努力啊。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(int k, vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    if(prices.empty()) return 0;
    //用II的解法优化k &amp;gt; n / 2的情况
    int n = prices.size();
    if(k &amp;gt; n / 2){
        int sum = 0;
        for(int i = 1; i &amp;lt; n; ++i){
            if(prices[i] &amp;gt; prices[i-1]) sum += prices[i] - prices[i-1];
        }
        return sum;
    }
    //初始化全局变量和局部变量
    vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; local(n+1, vector&amp;lt;int&amp;gt;(k+1, 0));
    vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; global(n+1, vector&amp;lt;int&amp;gt;(k+1, 0));

    for(int i = 1; i &amp;lt; n; ++i){
        int diff = prices[i] - prices[i-1];
        for(int j = 1; j &amp;lt; k + 1; ++j){
            //更新局部变量
            local[i][j] = max(global[i-1][j-1] + max(0, diff), local[i-1][j] + diff);
            //更新全局变量
            global[i][j] = max(global[i-1][j], local[i][j]);
        }
    }
    return global[n - 1][k];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有一种解法，类似于 ||| 中后一种解法一样，区别是我们这次要用 2K 个变量来记录 K 次交易。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int maxProfit(int k, vector&amp;lt;int&amp;gt; &amp;amp;prices) {
    // write your code here
    if(prices.empty()) return 0;
    //用II的解法优化k &amp;gt; n / 2的情况
    int n = prices.size();

    if(k &amp;gt; n / 2){
        int sum = 0;
        for(int i = 1; i &amp;lt; n; ++i){
            if(prices[i] &amp;gt; prices[i-1]) sum += prices[i] - prices[i-1];
        }
        return sum;
    }
    //初始化买卖股票后剩余金钱的数组
    vector&amp;lt;int&amp;gt; release (k + 1, 0);
    vector&amp;lt;int&amp;gt; hold    (k + 1, INT_MIN);


    for(int i = 0; i &amp;lt; n; i++){
        for(int j = 1; j &amp;lt; k+1; ++j){
            //卖出第j笔交易，所剩余的钱
            release[j] = max(release[j], hold[j] + prices[i]);
            //买入第j笔交易，所剩余的钱
            hold[j] = max(hold[j], release[j-1] - prices[i]);
        }
    }
    return release[k];
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 克隆二叉树</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-09-02/</link>
      <pubDate>Sun, 09 Oct 2016 20:57:46 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-09-02/</guid>
      <description>&lt;p&gt;题目链接：&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/clone-binary-tree/&#34;&gt;LintCode 克隆二叉树&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;很简单，就是在对二叉树进行遍历的时候重新生成节点就可以了，前序、中序或者后序遍历都可以，我用的是前序遍历：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
* Definition of TreeNode:
* class TreeNode {
* public:
*     int val;
*     TreeNode *left, *right;
*     TreeNode(int val) {
*         this-&amp;gt;val = val;
*         this-&amp;gt;left = this-&amp;gt;right = NULL;
*     }
* }
*/
class Solution {
public:
/**
 * @param root: The root of binary tree
 * @return root of new tree
 */
TreeNode* cloneTree(TreeNode *root) {
    // Write your code here
    if (root == NULL) return NULL;
    TreeNode *newRoot = new TreeNode(root-&amp;gt;val);
    if (root-&amp;gt;left) newRoot-&amp;gt;left = cloneTree(root-&amp;gt;left);
    if (root-&amp;gt;right) newRoot-&amp;gt;right = cloneTree(root-&amp;gt;right);
    return newRoot;
    }
};
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>