<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello world!</title>
    <link>https://shiweiHou.github.io/</link>
    <description>Recent content on Hello world!</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 17 Dec 2016 15:03:25 +0800</lastBuildDate>
    <atom:link href="https://shiweiHou.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ex9-10.The end</title>
      <link>https://shiweihou.github.io/machinelearning/exend/</link>
      <pubDate>Sat, 17 Dec 2016 15:03:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/exend/</guid>
      <description>&lt;p&gt;第九周和第十周没有再讲具体的算法了，而是介绍了一些机器学习变种的算法。第九周介绍了如何在非常非常大的数据下进行机器学习，以及流算法，第十周则介绍了流水线工作，类似于模块化，将一个大的机器学习任务分解成几个小部分，然后分析哪部分可以进行改进，哪部分不需要花费太多的人力物力物改进。&lt;/p&gt;

&lt;p&gt;总之，感觉正学习的渐入佳境，突然就没了，总有一种意犹未尽的感觉。机器学习只是比较有兴趣，然而现在我用到的机会却不是很多，希望可以继续学习下去，获得更多的知识。&lt;/p&gt;

&lt;p&gt;再次感谢Andrew Ng的无私奉献，还有Coursera平台提供了这次机会，再次感谢！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>C&#43;&#43;按行读取文本字符串</title>
      <link>https://shiweihou.github.io/cplusplus/C&#43;&#43;%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%AD%97%E7%AC%A6%E4%B8%B2/</link>
      <pubDate>Tue, 13 Dec 2016 16:55:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/cplusplus/C&#43;&#43;%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%AD%97%E7%AC%A6%E4%B8%B2/</guid>
      <description>&lt;p&gt;我们有一个文本文件，文件信息都是按行存储的，并且每行都包含很多字符串，例如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;you are my girl!
This is number 123456!
Do you like me? aha
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似于这样，我们有时候可能希望不仅仅读取文本里的字符串，还希望能够按行读取，因为有时候文本里的内容是相关的，即每行的信息大同小异，但每行间又会有点小区别，或者我们希望处理某特定一行的数字等等。这时我们希望可以每次读取一行，然后把每行的字符串都拆分出来，我们就可以按照下面的操作：
std::fstream
C++ 有一个头文件&lt;code&gt;&amp;lt;fstream&amp;gt;&lt;/code&gt;,里面有三个类成员 &lt;code&gt;fstream ifstream ofstream&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ifstream：定义读取的文件，进行读操作，而无法进行写操作&lt;/li&gt;
&lt;li&gt;ofstream：定义要写的文件，进行写操作，而无法进行读操作&lt;/li&gt;
&lt;li&gt;fstream：定义读写文件，既可以进行读操作，也可以进行写操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一行一行的读取文件内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;fstream&amp;gt;

std::string fileName = &amp;quot;line.ply&amp;quot;;
std::ifstream input_file(fileName.c_str());
std::string line;
if (input_file.is_open()) {
    while (getline(input_file, line)) {
        cout &amp;lt;&amp;lt; line &amp;lt;&amp;lt; endl; // 文件每一行的数据都保存在line中
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上式中&lt;code&gt;getline&lt;/code&gt;函数，就将每一行的内容都放入line中，直到读取到末尾。&lt;/p&gt;

&lt;p&gt;那么现在我们得到每行的数据了，如何得到每行单独的字符串呢，这时候就要用到&lt;code&gt;sstream&lt;/code&gt;，它和&lt;code&gt;fstream&lt;/code&gt;类似，不过前者是对文件进行操作，是文件IO，后者是对string对象操作，就像string是一个IO流一样。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;istringstream：从string读取数据&lt;/li&gt;
&lt;li&gt;ostringstream：向string写入数据&lt;/li&gt;
&lt;li&gt;stringstream：对string既可以写，也可以读&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对上面得到的一行数据进行操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include&amp;lt;sstream&amp;gt;
std::istringstream text(line);
std::string content;
while (text &amp;gt;&amp;gt; content) {
    do something with content.//这时我们就将一行的数据拆分成单个的字符串，就可以进行操作了
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex8.Anomaly Detection and Recommender Systems</title>
      <link>https://shiweihou.github.io/machinelearning/ex8/</link>
      <pubDate>Sat, 03 Dec 2016 18:03:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex8/</guid>
      <description>

&lt;p&gt;这节课主要讲了如何进行异常检测，介绍了推荐系统的基本概念和如何搭建一个推荐系统，都是一些基础概念，比较简单。
已经第九周了，还有两周的课就结束了，↖加油(^ω^)↗&lt;/p&gt;

&lt;h2 id=&#34;estimategaussian-m&#34;&gt;estimateGaussian.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [mu sigma2] = estimateGaussian(X)
%ESTIMATEGAUSSIAN This function estimates the parameters of a 
%Gaussian distribution using the data in X
%   [mu sigma2] = estimateGaussian(X), 
%   The input X is the dataset with each n-dimensional data point in one row
%   The output is an n-dimensional vector mu, the mean of the data set
%   and the variances sigma^2, an n x 1 vector
% 
% Useful variables
[m, n] = size(X);
% You should return these values correctly
mu = zeros(n, 1);
sigma2 = zeros(n, 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the mean of the data and the variances
%               In particular, mu(i) should contain the mean of
%               the data for the i-th feature and sigma2(i)
%               should contain variance of the i-th feature.
%
% X 是m行n列，sum（X)将每一列的和加起来然后除去行数m得到一个1*n维的行向量，进行转置，得到列向量
mu = (sum(X)/m)&#39;;

mu2 = mu&#39;;%将mu变成原来1*n维的行向量[1 2 3 .. n]
%mu2(ones(m,1),:)是将一个1*n维的行向量变成m*n的矩阵，即复制了m次，此时mu2就变成了
%    [u1,u2,u3,...,un
%     u1,u2,u3,...,un
%     .
%     .
%     u1,u2,u3,...,un
sigma2 = (sum((X - mu2(ones(m,1),:)).^2) / m )&#39;;
% =============================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;selectthreshold-m&#34;&gt;selectThreshold.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [bestEpsilon bestF1] = selectThreshold(yval, pval)
%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting
%outliers
%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best
%   threshold to use for selecting outliers based on the results from a
%   validation set (pval) and the ground truth (yval).
%

bestEpsilon = 0;
bestF1 = 0;
F1 = 0;

stepsize = (max(pval) - min(pval)) / 1000;
for epsilon = min(pval):stepsize:max(pval)

    % ====================== YOUR CODE HERE ======================
    % Instructions: Compute the F1 score of choosing epsilon as the
    %               threshold and place the value in F1. The code at the
    %               end of the loop will compare the F1 score for this
    %               choice of epsilon and set it to be the best epsilon if
    %               it is better than the current choice of epsilon.
    %               
    % Note: You can use predictions = (pval &amp;lt; epsilon) to get a binary vector
    %       of 0&#39;s and 1&#39;s of the outlier predictions

    predictions = (pval &amp;lt; epsilon);
    fp = sum((predictions == 1) &amp;amp; (yval == 0));
    tp = sum((predictions == 1) &amp;amp; (yval == 1));
    fn = sum((predictions == 0) &amp;amp; (yval == 1));
    prec = tp / (tp + fp);
    rec = tp / (tp + fn);
    F1 = 2 * prec * rec / (prec + rec);
    % =============================================================
    if F1 &amp;gt; bestF1
       bestF1 = F1;
       bestEpsilon = epsilon;
    end
end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;coficostfunc-m&#34;&gt;cofiCostFunc.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...
                              num_features, lambda)
%COFICOSTFUNC Collaborative filtering cost function
%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...
%   num_features, lambda) returns the cost and gradient for the
%   collaborative filtering problem.
%

% Unfold the U and W matrices from params
X = reshape(params(1:num_movies*num_features), num_movies, num_features);
Theta = reshape(params(num_movies*num_features+1:end), ...
                num_users, num_features);


% You need to return the following values correctly
J = 0;
X_grad = zeros(size(X));
Theta_grad = zeros(size(Theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost function and gradient for collaborative
%               filtering. Concretely, you should first implement the cost
%               function (without regularization) and make sure it is
%               matches our costs. After that, you should implement the 
%               gradient and use the checkCostFunction routine to check
%               that the gradient is correct. Finally, you should implement
%               regularization.
%
% Notes: X - num_movies  x num_features matrix of movie features
%        Theta - num_users  x num_features matrix of user features
%        Y - num_movies x num_users matrix of user ratings of movies
%        R - num_movies x num_users matrix, where R(i, j) = 1 if the 
%            i-th movie was rated by the j-th user
%
% You should set the following variables correctly:
%
%        X_grad - num_movies x num_features matrix, containing the 
%                 partial derivatives w.r.t. to each element of X
%        Theta_grad - num_users x num_features matrix, containing the 
%                     partial derivatives w.r.t. to each element of Theta
%

t1 = X*Theta&#39;;
t1 = t1 - Y;
t1 = t1.^2;
J = sum(sum(R.*t1)) / 2;
% with regularization J
J = J + lambda / 2 * ( sum( sum (Theta.^2) )  + sum( sum (X.^2) ));
X_grad = R.*(X*Theta&#39; - Y) * Theta;
Theta_grad = (R.*(X*Theta&#39; - Y))&#39; * X;
% with regularization   
X_grad = X_grad + lambda * X;
Theta_grad = Theta_grad + lambda * Theta;
% =============================================================
grad = [X_grad(:); Theta_grad(:)];

end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex7.K-means Clustering and Principal Component Analysis</title>
      <link>https://shiweihou.github.io/machinelearning/ex7/</link>
      <pubDate>Tue, 29 Nov 2016 11:18:44 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex7/</guid>
      <description>

&lt;p&gt;这节课主要讲了非监督学习的两个：K-means 和 Principal Component Analysis。K-means算法思想很简单，就是给你一堆数据，你需要在这无序的数据中将类似的数据聚合起来，分成K类，具体要分几类，可以通过 Elbow method （肘部法则）或者根据实际需求来确定。PCA算法通俗意义来讲，是降维操作。如果有个数据特征维数很大，例如100000维，其实在这么多的特种中，很多特征都是由相互联系的，通过PCA算法，就可以将原来维数很大数据特征，降低到我们容易进行处理的维数。&lt;/p&gt;

&lt;p&gt;PCA有几个好处： 降低存储容量，通过降维，提高学习算法运行速度。特别的，通过降维到2D或者3D，我们可以显示的观察特征，利于处理。但是如果想要通过PCA去降维来避免学习算法的过拟合问题，是不推荐的，或者不可取的。&lt;/p&gt;

&lt;h2 id=&#34;findclosestcentroids-m&#34;&gt;findClosestCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;m = size(X,1);
for i = 1 : m
    %二维数组，第i行用X(i,:)表示，不是X(i)
    dis = sum((X(i,:) - centroids(1,:)).^2);
    idx(i) = 1;
    for k = 1 : K
        dis2 = sum((X(i,:) - centroids(k,:)).^2);
        if  dis &amp;gt;= dis2
            dis = dis2;
            idx(i) = k;
        end
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computecentroids-m&#34;&gt;computeCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for k = 1 : K
    centroid = zeros(1,n);
    Ck = 0;
    for i = 1 : m
        if idx(i) == k
            centroid = centroid + X(i,:);
            Ck = Ck + 1;
        end
    end
    centroid = centroid / Ck;
    centroids(k,:) = centroid;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kmeansinitcentroids-m&#34;&gt;kMeansInitCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;randidx = randperm(size(X,1));
centroids = X(randidx(1:K),:);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pca-m&#34;&gt;pca.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;eigenvector = X&#39;* X / m;
[U,S,~] = svd(eigenvector);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;projectdata-m&#34;&gt;projectData.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;U_reduce = U(:,1:K);
Z = X * U_reduce;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;recoverdata-m&#34;&gt;recoverData.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;X_rec = Z * U(:,1:K)&#39;;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex6.Support Vector Machines</title>
      <link>https://shiweihou.github.io/machinelearning/ex6/</link>
      <pubDate>Fri, 25 Nov 2016 20:52:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex6/</guid>
      <description>

&lt;p&gt;这节讲到了SVM，Andrew Ng大神主要简单介绍了下核函数和SVM一些参数的影响。因为Ng大牛介绍的比较简单，所以看完自己又去网上百度了下，系统了了解了下。拙见就不在这发了，大家有兴趣的可以自己去百度。其实SVM就是个分类器，利用特定的核函数将在原空间维度线性不可分问题映射到高维，使之变成线性可分。另外学习这节课的时候又对偏差和方差有点迷，在知乎上找到一篇答案，比较优秀，链接在此：&lt;a href=&#34;https://www.zhihu.com/question/27068705&#34;&gt;机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;gaussian-kernel&#34;&gt;Gaussian Kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%很简单，就是按照公式计算高斯核内积，难度0
sim = -sum( (x1 - x2).^2 ) /  2 / sigma^2;
sim = exp(sim);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dataset3params-m&#34;&gt;dataset3Params.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%选择出最好的一对 C sigma
C_final = C;
sigma_final = sigma;
min_error = size(yval,1);
% C 和 sigma 所有的可能
para = [0.01 0.03 0.1 0.3 1 3 10 30];
m = size(para,2);
% 将 C 和 sigma 所有的可能都训练测试一遍，找到效果最好的那对
for i = 1 : m
    for j = 1 : m
        C = para(i);
        sigma = para(j);
        model = svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));
        pre = svmPredict(model,Xval);
        cur_error = mean(double(pre ~= yval));
        if cur_error &amp;lt; min_error
            min_error = cur_error;
            C_final = C;
            sigma_final = sigma;
        end
    end
end
C = C_final;
sigma = sigma_final;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;processemail-m&#34;&gt;processEmail.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%就是找到单词出现的下标位置，然后已列向量存储下来
for i = 1 : length(vocabList)
    if strcmp(str, vocabList{i}) == 1
        word_indices = [word_indices ; i];
        break;
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;emailfeatures-m&#34;&gt;emailFeatures.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%这个就更简单了，简单的说就是将所有出现的单词所在的下标位置，赋值1
for i = 1 : size(word_indices,1)
    x(word_indices(i)) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在已经学习到第七周了，发现越高深的算法，其实实现起来越简单，因为现有的函数库都将其实质内容都包含隐藏起来了，你只需要实现相应的输入参数就可以了。说明发明一个算法是最难的，但学会用一个算法，就太简单了。
另在原有的代码有一个小bug，就是没办法显示Example Dataset 2的边界线，是visualizeBoundary.m出了点问题，课程助教也给了解决方法，及时将原有的&lt;code&gt;contour(X1, X2, vals, [0 0], &#39;Color&#39;, &#39;b&#39;);&lt;/code&gt;改为&lt;code&gt;contour(X1, X2, vals, [1 1], &#39;b&#39;);&lt;/code&gt;就可以了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pcl_boundary</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_boundary/</link>
      <pubDate>Fri, 18 Nov 2016 13:41:18 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_boundary/</guid>
      <description>&lt;p&gt;PCL 里又找到一个牛逼的函数：BoundaryEstimation，用来发现三维点云的边界的。因为一直在做三维重建，一开始的想法是将三维点云投影到二维上，接着将二维的点云进行网格划分，根据落在网格内的点的数量设置为灰度值，这样我们就可以得到一个灰度图。接着就是利用图像处理中的方法，先进行二值化，然后用canny算子找边界，用hough变换找直线，最后再反投射到三维上。后来试了下，这个方法不仅复杂，而且进行两次三维到二维之间的变化，误差太大。后来在PCL里找到BoundaryEstimation这个类，能够自动的找到边界，很强大，但最好也是在二维下做，意思是先找到面，在面上找边界。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void LasFunction::boundaryFind(void) {
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();
    pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt;::Ptr tree = boost::shared_ptr&amp;lt;pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt; &amp;gt;(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;);
    // 计算法向量，因为边界点查找需要用到法向量
    pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;::Ptr normals(new pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;);
    pcl::NormalEstimation&amp;lt;pcl::PointXYZ, pcl::Normal&amp;gt; normal_estimator;
    normal_estimator.setSearchMethod(tree);
    normal_estimator.setInputCloud(cloud);
    normal_estimator.setKSearch(30);
    normal_estimator.compute(*normals);

    pcl::PointCloud&amp;lt;pcl::Boundary&amp;gt; boundaries;
    pcl::BoundaryEstimation&amp;lt;pcl::PointXYZ, pcl::Normal, pcl::Boundary&amp;gt; est;
    est.setInputCloud(cloud);
    est.setInputNormals(normals);
    est.setSearchMethod(pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;::Ptr(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;));
    //50效果比较好，这个数值的意思我的理解是根据点周围这么多个点来进行判断我们找到的点究竟是不是边界点
    est.setKSearch(100);
    est.compute(boundaries);
    if (boundaries.points.empty()) std::cout &amp;lt;&amp;lt; &amp;quot;error&amp;quot; &amp;lt;&amp;lt; std::endl;
    else {
        pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr boundaryPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
        for (int i = 0; i &amp;lt; cloud-&amp;gt;points.size(); ++i) {
        //当初在这卡了好久，因为虽然找到了边界点，但无法直接输出，它里面是uint8_t类型的，如果是边界点，它保存1，如果不是，保存0.若想在计算机里显式的显示出来，需要进行强制类型转换，变成int型才可以
            uint8_t  x = boundaries.points[i].boundary_point;
            int a = static_cast&amp;lt;int&amp;gt;(x);
            if (1 == a) boundaryPoints-&amp;gt;push_back(cloud-&amp;gt;points[i]);
        }
        std::cout &amp;lt;&amp;lt; boundaryPoints-&amp;gt;points.size() &amp;lt;&amp;lt; std::endl;
        std::cout &amp;lt;&amp;lt; &amp;quot;boundary is ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        radiusFilter(boundaryPoints);
        pcl::io::savePCDFile(&amp;quot;./line9/boundaryPlane.pcd&amp;quot;, *boundaryPoints);
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总之，这个类给了我很大的方便，PCL果然在一直进步，很牛逼的一个库，需要我继续学习下去&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PCL SampleConsensusMode</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_ransac2/</link>
      <pubDate>Fri, 18 Nov 2016 13:19:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_ransac2/</guid>
      <description>

&lt;p&gt;前面那篇用PCL里的ransac模型进行面提取，其实严格意义来讲并不是真正的RANSAC类，而是利用的SACSegmentation类，然后在类。里面用SACMODEL_PLANE模型进行面提取。后来又找到PCL里真正的RANSAC函数 SampleConsensusMode，遂用这个实现了一下，发现效果要比前面的好一些，并且不仅在提面上效果好，在提线的表现上也非常好。&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelplane&#34;&gt;SampleConsensusModelPlane&lt;/h2&gt;

&lt;p&gt;这个用来提取面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelPlane(void) {
    // ransac 提取出来一个最大的平面
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    // 读取文件，将点云数据放入cloud中
    cloud = readFile();
    // 原始点云数量，用来判断何时停止面的提取
    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    // 当剩下的点云数量比原始点云数量大于10%时时，继续进行面的提取
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.1 * rawPoints) {

        //确定提取模型为面，点云类型要和前面的点云类型一致
        pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_p(new pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_p);
        //这个Threshold的意思是，同一平面的点相邻距离不能超过0.1m，或者是已0.1m为基准进行搜索，不能超过它。我试过用1 2等参数进行提取，发现提取出来的不仅仅是一个面了，还有高度，就像是一个立方体一样。所以最好小一些，可以根据具体的点云数据进行设置
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        //这个函数的意思是将inputPoints中，前面提取到的面（索引保留在了inliers中了），复制到点云out中，这时out中存的就是前面提取的面的点
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //建立析取器，因为我们需要不停的迭代，需要将上次提取出来的面从原来的点云中去除
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        //因为前面用到的索引是vector类型的，而析取器要求的是PointIndices，所以只要新建个，然后将上面的索引复制给它就行了
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        //只有设置为true，才可以将包含在inlier中的下标的点从原来的点云中去除
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);

        std::cout &amp;lt;&amp;lt; &amp;quot;ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;SampleModelPlane&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以提取面之外，这个函数还有个变形，就是提取直线，效果也是非常好的&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelline&#34;&gt;SampleConsensusModelLine&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelLine(void) {
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();

    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    std::multimap&amp;lt;double, double&amp;gt; mapKb;
    //和上面的函数几乎没有什么区别，只是将SampleConsensusModelPlane变成SampleConsensusModelLine
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.05 * rawPoints) {
        pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_l(new pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_l);
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //析取
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);


        std::cout &amp;lt;&amp;lt; &amp;quot;ok2.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;./line1/SampleModelLine&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }

    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为用的是1.8版本的PCL，里面实现了好多很需要的函数，所以功能还是很强大，几乎你能想到的都有，只是因为是通用的，所以可能在精度准确度上没有那么高，需要开发者自己再做进一步调整&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex5.Regularized Linear Regression and Bias v.s.Variance</title>
      <link>https://shiweihou.github.io/machinelearning/ex5/</link>
      <pubDate>Thu, 17 Nov 2016 21:15:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex5/</guid>
      <description>

&lt;p&gt;这个练习其实很简单，基本上都是以前练习的代码，然后让你再重写一遍。这次编程作业的主要作用就是让我们直观的看到不同大小的训练集及不同大小的lambda对拟合效果的影响。具体有以下几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature太少或者维数过低，会造成高偏差(high bias)，即欠拟合,解决方法是增加feature数，可以是维数增加或者feature数增加（polynomial features)或者降低lambda（正则化项）,feature数增加的前提示增加的feature必须和已有feature是相互独立的，不然没有任何意义，只能增加维数了&lt;/li&gt;
&lt;li&gt;类似于上条，如果feature过多或者feature的维数过高，会造成高方差(high variance),即过拟合。解决方法是减少feature数，将不是相互独立的feature去掉，或者降低维数，或者增加正则化项，降低维数对theta参数的影响。还有一个很有用的方法是增加训练集样本，来解决过拟合问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;regularized-linear-regression-cost-function-and-gradient&#34;&gt;Regularized linear regression cost function and Gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%非矩阵形式
for i = 1 : m
    h = theta(1) * X(i,1) + theta(2) * X(i,2);
    J = J +  (h - y(i))^2 / 2 / m;
end
J = J + lambda / 2 / m * theta(2)^2;
%矩阵形式
J = sum((X * theta - y).^2) / 2 / m + lambda / 2 /m * (sum(theta.^2) - theta(1)^2); 

theta1 = theta;
theta1(1) = 0;
grad = (X&#39; * (X * theta - y))/m + lambda / m * theta1;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;learning-curves&#34;&gt;Learning curves&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% ---------------------- Sample Solution ----------------------
for i = 1 : m
    theta = trainLinearReg(X(1:i,:), y(1:i,:), lambda);
    [error_train(i),grad] = linearRegCostFunction(X(1:i,:), y(1:i,:), theta, 0);
    [error_val(i),grad] = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个步骤中变得很笨，这个步骤是要让我们形式化的看到不同的训练集大小对误差的影响，我在计算训练误差的时候还好，但在计算error_val交叉集误差的时候，也学训练集那样只用Xval和yval的前i个样本，其实是要全部的样本。&lt;/p&gt;

&lt;h2 id=&#34;polynomial-regression&#34;&gt;Polynomial regression&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : p
    X_poly(:,i) = X.^i;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分主要让我们观察不同的feature polynomial对误差的影响，意思是原来的训练集X只有一列，然后我们根据p的值，另外增加p-1列，其中第2列的值为第一列值的平方，第3列的值为第一列值的立方，以此类推&lt;/p&gt;

&lt;h2 id=&#34;selecting-λ-using-a-cross-validation-set&#34;&gt;Selecting λ using a cross validation set&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : length(lambda_vec)
    lambda = lambda_vec(i);
    theta = trainLinearReg(X, y, lambda);
    error_train(i) = linearRegCostFunction(X, y, theta, 0);
    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分就和Learning curves这部分很像，所不同的是Learning curves部分看训练集大小对误差的影响，这次呢，是看不同的lambda λ对误差的影响，训练集和交叉集大小是固定的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex4.Neural Networks Learning</title>
      <link>https://shiweihou.github.io/machinelearning/ex4/</link>
      <pubDate>Wed, 09 Nov 2016 10:49:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex4/</guid>
      <description>

&lt;p&gt;Neural Networks Learning：实现最简单的一个神经网络学习系统，实现反向传播和正向传播，并利用数值计算误差来检测神经网络算法是否可行，或者说cost function是否计算正确。&lt;/p&gt;

&lt;h2 id=&#34;feedforward-and-cost-function&#34;&gt;Feedforward and cost function&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through the
%               following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%         variable J. After implementing Part 1, you can verify that your
%         cost function computation is correct by verifying the cost
%         computed in ex4.m
a1 = [ones(m,1) X]; % 5000 * 401
Z2 = a1 * Theta1&#39;;  % 5000 * hidden_layer
a2 = sigmoid(Z2);   % sigmoid
a2 = [ones(size(a2,1),1) a2]; % 5000 * (hidden_layer + 1)
Z3 = a2 * Theta2&#39;;            % 5000 * 10
a3 = sigmoid(Z3);
[max_a3, index] = max(a3,[],2);

I = eye(num_labels); 
Y = zeros(m, num_labels);  % 5000 * 10
for i = 1:m
    Y(i,:) = I(y(i),:);  %  让每一个样例第y(i)列变成1，即将样本输出变成行向量，而不是列向量，每一行为1的列的值即为样本的输出
end
%J = sum(sum((-Y).*log(a3) - (1-Y).*log(1-a3),2))/m;
JJ = 0;
for i = 1 : m   
   JJ = JJ + sum( -1*Y(i,:).* log(a3(i,:)) - (1-Y(i,:)).*log(1-a3(i,:))); %对所有的样本进行相加，每个样本计算J，最后加起来
end
J = JJ / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-cost-function&#34;&gt;Regularized cost function&lt;/h2&gt;

&lt;p&gt;就是在前面计算出来的J的基础上，加上正则化项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%要去掉theta(1),第一个theta不需要参加
theta1 = Theta1(:,2:size(Theta1,2));
theta2 = Theta2(:,2:size(Theta2,2));
reg = lambda * ( sum ( sum ( theta1.^2) ) + sum ( sum ( theta2.^2) ));
reg = reg / 2 / m;
J = J + reg;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sigmoid-gradient&#34;&gt;Sigmoid gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function g = sigmoidGradient(z)
%SIGMOIDGRADIENT returns the gradient of the sigmoid function
%evaluated at z
%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function
%   evaluated at z. This should work regardless if z is a matrix or a
%   vector. In particular, if z is a vector or matrix, you should return
%   the gradient for each element.

g = zeros(size(z));
g = sigmoid(z).* (1 - sigmoid(z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sigma3 = a3-Y;
sigma2 = (sigma3*Theta2).*sigmoidGradient([ones(size(Z2, 1), 1) Z2]);
sigma2 = sigma2(:, 2:end);

delta_1 = (sigma2&#39;*a1);
delta_2 = (sigma3&#39;*a2);

p1 = (lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];
p2 = (lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];
Theta1_grad = delta_1./m + p1;
Theta2_grad = delta_2./m + p2;

grad = [Theta1_grad(:) ; Theta2_grad(:)];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;神经网络算法比较简单，就是计算过程稍微复杂了点，特别是牵涉到里面的矩阵运算的时候，比较麻烦。第一次做的时候感觉无从下手，后来又做了一遍，才感觉稍微懂点。思路都懂，就是写出来比较困难，还需努力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex3.Multi-class Classification and Neural Networks</title>
      <link>https://shiweihou.github.io/machinelearning/ex3/</link>
      <pubDate>Thu, 03 Nov 2016 19:56:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex3/</guid>
      <description>

&lt;p&gt;Multi-class Classification&lt;/p&gt;

&lt;p&gt;多分类，其实和前面的二分类，即Logistics regression差不多，只不过因为分类结果有很多个，对于特定的输入，会产生多个输出，在这多个输出里面找到概率最大的那个输出，即为分类答案。&lt;/p&gt;

&lt;h2 id=&#34;lrcostfunction-m&#34;&gt;lrCostFunction.m&lt;/h2&gt;

&lt;p&gt;和第二个练习的一样，就是指导文档里要求我们向量化，即矩阵化求解。前面的练习我们已经做到了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J = (0-y).*log(J1) - (1-y).* log (1-J1);
J = sum(J);
J = J / m;
J = J + (lambda / 2 / m) * (sum(theta.^2) - theta(1)^2);
% 求 Gradient 
grad = X&#39; *  (J1 - y ) / m;
tmp = theta;
tmp(1) = 0;
grad = grad + (lambda / m) * tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;one-vs-all-classification&#34;&gt;One-vs-all Classification&lt;/h2&gt;

&lt;p&gt;其实多分类也是一个“二分类”问题，只不过每次我们都是选取其中概率最大的那个作为输出“1”，其余的都作为“0”。&lt;strong&gt;oneVsAll.m&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for c = 1 : num_labels
    %yc = (y == c);
    initial_theta = zeros(n + 1, 1);
    options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
    [theta] = ...
         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...
                 initial_theta, options);
    all_theta(c,:) = theta&#39;;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;predictonevsall-m&#34;&gt;predictOneVsAll.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% max函数，得到tmp矩阵中每一行最大的那个列下标
% 在本题中，列下标即为分类结果
[maximum, index] = max(tmp, [], 2);
% 如果下标是10，我们就用0表示
index(index == 10) = 0;
p = index;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;这节课Andrew还简单的介绍了下神经网络，并且给出了如何利用神经网络来分类。编程题中Andrew已经给好了训练好的Theta值，只需要我们能根据训练好的Theta值和输入的X值，得到Y值就行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
%加上第一项1（bias unit)，因为X是m * n的，每个样本的特征在一行，所以加上的就是 m*1的一列，即在每一行的前面加上了个1
X = [ones(m, 1) X];
for i = 1 : m
    XX = X(i:i,:);
    z2 = Theta1 * XX&#39;;
    z2 = sigmoid(z2);
    %row = size(z2,1)
    %加上第一项 1，因为z2是 n*1的，即第i个样本的z2都是单独一列，是个列向量，因此需要加上一行1，即增加一行1
    z22 = [ones(1,1);z2];
    z3 = Theta2 * z22;
    % a3 即为输出结果
    a3 = sigmoid(z3);
    [maxInd, index] = max(a3,[],1);
    if index == 0
        index = 10;
    end
    p(i) = index;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在每一层网络中计算的时候，都要注意要加上一项1作为bias unit&lt;/li&gt;
&lt;li&gt;注意在前面一层输出和Theta积计算完之后，不要忘记做sigmoid操作&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ex2.Logistics Regression</title>
      <link>https://shiweihou.github.io/machinelearning/ex2/</link>
      <pubDate>Thu, 03 Nov 2016 18:34:49 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex2/</guid>
      <description>

&lt;p&gt;第二次编程作业，逻辑回归，其实可以理解为二分类问题。&lt;/p&gt;

&lt;h2 id=&#34;sigmoid-function&#34;&gt;sigmoid function：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% 迭代实现
[row, col] = size(z); 
for i = 1:row
    for j = 1:col
        g(i,j) = 1 / (1 + exp(-z(i,j)));
    end
end

% 非迭代实现
g = 1.0 ./ (1.0 + exp(-z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cost-function-and-gradient&#34;&gt;Cost function and Gradient：&lt;/h2&gt;

&lt;p&gt;一开始我是用迭代做的，后来学习了第四课之后，我又回来将迭代的版本改为矩阵版本，看起来确实清晰了很多，以后尽量都用矩阵形式来做（好像这样也更符合matlab的要求）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 迭代版本
theta1 = theta&#39;;
[row,col] = size(theta1);
% 求Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    grad(j,1) = J2;
end
% 求 J，cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是矩阵版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J1 = sigmoid(X*theta);
% 求 J， cost function
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-logistic-regression&#34;&gt;Regularized logistic regression：&lt;/h2&gt;

&lt;p&gt;为什么要用到正则化呢？是因为有时候特征项我们选取的过多，有时候会出现过拟合问题。那么，一旦出现过拟合问题的时候，就会发生训练模型对训练集拟合非常好，但对新的数据就拟合偏差很大。这个时候，我们就需要用正则化项来惩罚参数，防止其过大，从而避免过拟合问题。同样，我一开始写的是迭代过程，后来又被我改成矩阵形式了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta1 = theta&#39;;
[row,col] = size(theta1);
% 求 Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    %对第一个参数 theta0 不进行正则化
    if j &amp;gt;= 2
        grad(j,1) = J2 + (lambda / m ) * theta(j);
    else
        grad(j,1) = J2;
    end
end
%求 J， cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
% 对第一个参数 theta0 不进行正则化
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;矩阵形式，和上面的logistics regression矩阵形式差不多，只需要注意不要对第一项进行处理即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
grad = grad + (lambda / m ) * theta;
grad(1) = grad(1) - (lambda / m ) * theta(1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对了，还有个predict函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : m
    XX = X(i:i,:);
    pp = sigmoid(XX*theta);
    if pp &amp;gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>利用PCL对三维点云进行面提取</title>
      <link>https://shiweihou.github.io/pointcloud/PCL_RANSAC/</link>
      <pubDate>Tue, 01 Nov 2016 21:15:04 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/PCL_RANSAC/</guid>
      <description>&lt;p&gt;因为要做三维重建，早上刚和老师讨论下基本流程，目前的想法是先提取出室内建筑物的线框结构，然后利用线框结构进行表面重建。后来有个师兄讲可以先利用PCL里RANSAC函数先进行面提取，所以就试了下。下面是主要代码，一些内容都写到注释里面去了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool LasFunction::ransac(void) {
    std::string fileName = &amp;quot;basementCut.las&amp;quot;;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud (new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    CLasOperator las;
    las.readLasFile(fileName.c_str());
    std::vector&amp;lt;Point3D&amp;gt; points = las.getPointData();
    for (size_t i = 0; i &amp;lt; points.size(); ++i) {
        pcl::PointXYZ onepoint;
        onepoint.x = points[i].x;
        onepoint.y = points[i].y;
        onepoint.z = points[i].z;
        cloud-&amp;gt;push_back(onepoint);
    }
    // Create the filtering object: downsample the dataset using a leaf size of 1cm
    // 用来进行降采样操作，应该就是用LeafSize进行，里面单位是以m为单位，在这么个立方体内，得到一个点
    pcl::VoxelGrid&amp;lt;pcl::PointXYZ&amp;gt; vg;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_filtered(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    vg.setInputCloud(cloud);
    vg.setLeafSize(0.1f, 0.1f, 0.1f);
    vg.filter(*cloud_filtered);
    std::cout &amp;lt;&amp;lt; &amp;quot;PointCloud before filtering has: &amp;quot; &amp;lt;&amp;lt; cloud-&amp;gt;points.size() &amp;lt;&amp;lt; &amp;quot; data points.&amp;quot; &amp;lt;&amp;lt; std::endl;
    std::cout &amp;lt;&amp;lt; &amp;quot;PointCloud after filtering has: &amp;quot; &amp;lt;&amp;lt; cloud_filtered-&amp;gt;points.size() &amp;lt;&amp;lt; &amp;quot; data points.&amp;quot; &amp;lt;&amp;lt; std::endl;


    //构建一个ransac分割器，inliers存储索引，即到底哪些个点是在ransac得到的目标内，setModelType是用来表示ransac分割到底是分割成面或者分割成线或者分割成其它形状，coefficients用来存储系数，即加入表示的是个平面的话，点应该在类似于 a*x + b*y + c*z + d = 0内

    pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);
    pcl::PointIndices::Ptr inliers (new pcl::PointIndices);
    pcl::SACSegmentation&amp;lt;pcl::PointXYZ&amp;gt; sac;
    sac.setInputCloud(cloud);
    sac.setMethodType(pcl::SAC_RANSAC);
    sac.setModelType(pcl::SACMODEL_PLANE);
    //sac.setModelType(pcl::SACMODEL_LINE);
    sac.setDistanceThreshold(0.01);
    sac.setMaxIterations(100);
    sac.setProbability(0.9);


    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_plane(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;());
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud_f(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;());
    int num = 1;

    // 保留原始点云中点云数量多少
    int nr_points = (int)cloud_filtered-&amp;gt;points.size();

    // 可以生成多个面，直到剩余点云中点的数量小于原始点云数量的 1/10
    while (cloud_filtered-&amp;gt;points.size() &amp;gt; 0.1 * nr_points) {
        sac.setInputCloud(cloud_filtered);
        sac.segment(*inliers, *coefficients);
        if (inliers-&amp;gt;indices.size() == 0) {
            std::cout &amp;lt;&amp;lt; &amp;quot;Could not estimate a planar nodel for the given dataset.&amp;quot; &amp;lt;&amp;lt; std::endl;
            break;
        }
        // 构建析取模型
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; extract;
        extract.setInputCloud(cloud_filtered);
        extract.setIndices(inliers);
        //如果为真，则前面放进去点除了索引里面的都会保留下来
        //否则，只保留下索引里面的点，通过filter函数将需要保留下的点放进去 
        extract.setNegative(false);     
        extract.filter(*cloud_plane);
        for (size_t i = 0; i &amp;lt; inliers-&amp;gt;indices.size(); ++i) {
            pcl::PointXYZ onePoint;
            onePoint.x = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].x;
            onePoint.y = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].y;
            onePoint.z = cloud_filtered-&amp;gt;points[inliers-&amp;gt;indices[i]].z;
            cloud_plane-&amp;gt;push_back(onePoint);
        }
        pcl::PCDWriter writer;
        std::string fileName = &amp;quot;ransacPlane&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        fileName += std::to_string(num);
        fileName += houzhui;
        writer.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName, *cloud_plane);
        std::cout &amp;lt;&amp;lt; fileName &amp;lt;&amp;lt; &amp;quot; is ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        ++num;

        //再设置为真，这样点云中除了索引里面的点，都会保留下来，这样我们就将前面提取出来的面里点去除掉了
        extract.setNegative(true);
        extract.filter(*cloud_f);
        cloud_filtered = cloud_f;
    }

    return true;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 分糖果</title>
      <link>https://shiweihou.github.io/lintcode/2016-10-31-01/</link>
      <pubDate>Mon, 31 Oct 2016 20:01:12 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-10-31-01/</guid>
      <description>&lt;p&gt;题目链接：&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/candy/&#34;&gt;分糖果&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;思路：&lt;/p&gt;

&lt;p&gt;一开始想的先排序，然后从低到高遍历，定义一个rat和pre，rat保存当前需要加的糖果数，pre为上一个rating，如果当前ratings[i] == pre， 就将rat置为1，不是就将rat++。。。。。后来发现不是这么做的，因为测试数据怎么都过不了。&lt;/p&gt;

&lt;p&gt;首先，不能排序！不能改变原有的顺序！题目的意思应该理解为“ 当前rating如果比其左右都高的话，其糖果个数不应该小于其左右的糖果个数”。 所以需要遍历两次，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先从左到右遍历一次，保证当右边的rating大于其左边的rating值时，糖果个数不小于左边的糖果值&lt;/li&gt;
&lt;li&gt;然后从右往左再遍历一次，保证当左边的rating大于其右边的rating值时，糖果个数不小于右边的糖果值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后所有的加起来就可以了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
    /**
     * @param ratings Children&#39;s ratings
     * @return the minimum candies you must give
     */
    int candy(vector&amp;lt;int&amp;gt;&amp;amp; ratings) {
        // Write your code here
        int sum = 0;
        int len = ratings.size();
        int *num = new int[len];

        num[0] = 1;
        // 从左往右
        for (int i = 1; i &amp;lt; len; ++i) {
            num[i] = ratings[i] &amp;gt; ratings[i-1] ? num[i-1] + 1 : 1;
        }
        //从右往左
        for (int i = len - 1; i &amp;gt; 0; --i) {
            num[i - 1] = ratings[i] &amp;lt; ratings[i-1] ? max(num[i] + 1, num[i - 1]) : num[i - 1];
        }
        for (int i = 0; i &amp;lt; len; ++i) sum += num[i];

        delete []num;
        return sum;
    }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;，&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pcl 1.8.0 ALL in one &#43; vs 2013</title>
      <link>https://shiweihou.github.io/pointcloud/pcl/</link>
      <pubDate>Mon, 31 Oct 2016 15:45:15 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl/</guid>
      <description>&lt;p&gt;最近需要做三维重建，要用到pcl函数库里带的 RegionGrowing 函数，官网上只有1.6.0的老版本安装包，而且貌似1.6.0版本里没有集成完全的 RegionGrowing 函数，遂找到1.8.0版本的PCL，进行处理。&lt;/p&gt;

&lt;p&gt;网上目前有很多根据源码利用Cmake来编译PCL的教程，后来在一个&lt;a href=&#34;http://unanancyowen.com/?p=2009&amp;amp;lang=en&#34;&gt;日本人的博客&lt;/a&gt;上找到PCL 1.8.0的 all-in-one，遂下载安装之。&lt;/p&gt;

&lt;p&gt;我的电脑是win10 64 + vs 2013， 于是下载的是PCL 1.8.0 All-in-one Installer MSVC2013 Win64 这个版本。&lt;/p&gt;

&lt;p&gt;安装过程：&lt;/p&gt;

&lt;p&gt;下载安装：下载并安装pcl_all_in_one 1.8.0,安装时选择“Add PCL to the system PATH for all users&amp;rdquo;选项，选择自己的安装路径，我是默认路径 C:\Program Files\PCL 1.8.0，然后一直点击下一步即可。注意在安装过程中会提示你安装OPENNI库，不要选择默认路径，要放在PCL路径下的3rdParty文件夹下，例如我的就安装在了 C:\Program Files\PCL 1.8.0\3rdParty\OpenNI2 这个文件夹内。&lt;/p&gt;

&lt;p&gt;配置环境变量：&lt;/p&gt;

&lt;p&gt;如果在上面的安装步骤中选择了“Add PCL to the system PATH for all users&amp;rdquo;选项，那么系统环境变量中会出现下图四个路径：如果没有的话可能就需要你手动添加了（一般出现这种情况的时候安装过程中就会提示“因为路径名太长无法写进去”之类的话） &lt;img src=&#34;https://raw.githubusercontent.com/shiweiHou/image/master/PCL-Install-image1.png&#34; alt=&#34;image1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在系统变量Path中加入：&lt;img src=&#34;https://raw.githubusercontent.com/shiweiHou/image/master/PCL-Install-image2.png&#34; alt=&#34;image2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设置包含目录：&lt;/p&gt;

&lt;p&gt;打开vs2013，新建win32控制台项目，选择X64平台，如果没有就新建个，然后在新建项目的属性管理器的中的VC++的包含目录中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Program Files\PCL 1.8.0\3rdParty\OpenNI2\Include\Win32;
C:\Program Files\PCL 1.8.0\3rdParty\Eigen\eigen3;
C:\Program Files\PCL 1.8.0\3rdParty\VTK\include\vtk-7.0;
C:\Program Files\PCL 1.8.0\3rdParty\FLANN\include\;
C:\Program Files\PCL 1.8.0\3rdParty\Qhull\include;
C:\Program Files\PCL 1.8.0\3rdParty\Boost\include\boost-1_61;
C:\Program Files\PCL 1.8.0\include\pcl-1.8;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在VC++的库目录中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c:\Program Files\PCL 1.8.0\lib;
c:\Program Files\PCL 1.8.0\3rdParty\Qhull\lib;
c:\Program Files\PCL 1.8.0\3rdParty\FLANN\lib;
c:\Program Files\PCL 1.8.0\3rdParty\Boost\lib;
c:\Program Files\PCL 1.8.0\3rdParty\VTK\lib;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置附加依赖项：
DEBUG模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcl_common_debug.lib
pcl_features_debug.lib
pcl_filters_debug.lib
pcl_io_debug.lib
pcl_io_ply_debug.lib
pcl_kdtree_debug.lib
pcl_keypoints_debug.lib
pcl_octree_debug.lib
pcl_outofcore_debug.lib
pcl_people_debug.lib
pcl_recognition_debug.lib
pcl_registration_debug.lib
pcl_sample_consensus_debug.lib
pcl_search_debug.lib
pcl_segmentation_debug.lib
pcl_surface_debug.lib
pcl_tracking_debug.lib
pcl_visualization_debug.lib
libboost_atomic-vc120-mt-gd-1_61.lib
libboost_chrono-vc120-mt-gd-1_61.lib
libboost_container-vc120-mt-gd-1_61.lib
libboost_context-vc120-mt-gd-1_61.lib
libboost_coroutine-vc120-mt-gd-1_61.lib
libboost_date_time-vc120-mt-gd-1_61.lib
libboost_exception-vc120-mt-gd-1_61.lib
libboost_filesystem-vc120-mt-gd-1_61.lib
libboost_graph-vc120-mt-gd-1_61.lib
libboost_iostreams-vc120-mt-gd-1_61.lib
libboost_locale-vc120-mt-gd-1_61.lib
libboost_log-vc120-mt-gd-1_61.lib
libboost_log_setup-vc120-mt-gd-1_61.lib
libboost_math_c99-vc120-mt-gd-1_61.lib
libboost_math_c99f-vc120-mt-gd-1_61.lib
libboost_math_c99l-vc120-mt-gd-1_61.lib
libboost_math_tr1-vc120-mt-gd-1_61.lib
libboost_math_tr1f-vc120-mt-gd-1_61.lib
libboost_math_tr1l-vc120-mt-gd-1_61.lib
libboost_mpi-vc120-mt-gd-1_61.lib
libboost_prg_exec_monitor-vc120-mt-gd-1_61.lib
libboost_program_options-vc120-mt-gd-1_61.lib
libboost_random-vc120-mt-gd-1_61.lib
libboost_regex-vc120-mt-gd-1_61.lib
libboost_serialization-vc120-mt-gd-1_61.lib
libboost_signals-vc120-mt-gd-1_61.lib
libboost_system-vc120-mt-gd-1_61.lib
libboost_test_exec_monitor-vc120-mt-gd-1_61.lib
libboost_thread-vc120-mt-gd-1_61.lib
libboost_timer-vc120-mt-gd-1_61.lib
libboost_type_erasure-vc120-mt-gd-1_61.lib
libboost_unit_test_framework-vc120-mt-gd-1_61.lib
libboost_wave-vc120-mt-gd-1_61.lib
libboost_wserialization-vc120-mt-gd-1_61.lib
flann_cpp_s-gd.lib
flann_s-gd.lib
flann-gd.lib
qhull_d.lib
qhull_p_d.lib
qhull_r_d.lib
qhullcpp_d.lib
qhullstatic_d.lib
qhullstatic_r_d.lib
vtkChartsCore-7.0-gd.lib
vtkCommonColor-7.0-gd.lib
vtkCommonComputationalGeometry-7.0-gd.lib
vtkCommonCore-7.0-gd.lib
vtkCommonDataModel-7.0-gd.lib
vtkCommonExecutionModel-7.0-gd.lib
vtkCommonMath-7.0-gd.lib
vtkCommonMisc-7.0-gd.lib
vtkCommonSystem-7.0-gd.lib
vtkCommonTransforms-7.0-gd.lib
vtkDICOMParser-7.0-gd.lib
vtkDomainsChemistry-7.0-gd.lib
vtkFiltersAMR-7.0-gd.lib
vtkFiltersCore-7.0-gd.lib
vtkFiltersExtraction-7.0-gd.lib
vtkFiltersFlowPaths-7.0-gd.lib
vtkFiltersGeneral-7.0-gd.lib
vtkFiltersGeneric-7.0-gd.lib
vtkFiltersGeometry-7.0-gd.lib
vtkFiltersHybrid-7.0-gd.lib
vtkFiltersHyperTree-7.0-gd.lib
vtkFiltersImaging-7.0-gd.lib
vtkFiltersModeling-7.0-gd.lib
vtkFiltersParallel-7.0-gd.lib
vtkFiltersParallelImaging-7.0-gd.lib
vtkFiltersProgrammable-7.0-gd.lib
vtkFiltersSMP-7.0-gd.lib
vtkFiltersSelection-7.0-gd.lib
vtkFiltersSources-7.0-gd.lib
vtkFiltersStatistics-7.0-gd.lib
vtkFiltersTexture-7.0-gd.lib
vtkFiltersVerdict-7.0-gd.lib
vtkGeovisCore-7.0-gd.lib
vtkIOAMR-7.0-gd.lib
vtkIOCore-7.0-gd.lib
vtkIOEnSight-7.0-gd.lib
vtkIOExodus-7.0-gd.lib
vtkIOExport-7.0-gd.lib
vtkIOGeometry-7.0-gd.lib
vtkIOImage-7.0-gd.lib
vtkIOImport-7.0-gd.lib
vtkIOInfovis-7.0-gd.lib
vtkIOLSDyna-7.0-gd.lib
vtkIOLegacy-7.0-gd.lib
vtkIOMINC-7.0-gd.lib
vtkIOMovie-7.0-gd.lib
vtkIONetCDF-7.0-gd.lib
vtkIOPLY-7.0-gd.lib
vtkIOParallel-7.0-gd.lib
vtkIOParallelXML-7.0-gd.lib
vtkIOSQL-7.0-gd.lib
vtkIOVideo-7.0-gd.lib
vtkIOXML-7.0-gd.lib
vtkIOXMLParser-7.0-gd.lib
vtkImagingColor-7.0-gd.lib
vtkImagingCore-7.0-gd.lib
vtkImagingFourier-7.0-gd.lib
vtkImagingGeneral-7.0-gd.lib
vtkImagingHybrid-7.0-gd.lib
vtkImagingMath-7.0-gd.lib
vtkImagingMorphological-7.0-gd.lib
vtkImagingSources-7.0-gd.lib
vtkImagingStatistics-7.0-gd.lib
vtkImagingStencil-7.0-gd.lib
vtkInfovisCore-7.0-gd.lib
vtkInfovisLayout-7.0-gd.lib
vtkInteractionImage-7.0-gd.lib
vtkInteractionStyle-7.0-gd.lib
vtkInteractionWidgets-7.0-gd.lib
vtkNetCDF-7.0-gd.lib
vtkNetCDF_cxx-7.0-gd.lib
vtkParallelCore-7.0-gd.lib
vtkRenderingAnnotation-7.0-gd.lib
vtkRenderingContext2D-7.0-gd.lib
vtkRenderingContextOpenGL-7.0-gd.lib
vtkRenderingCore-7.0-gd.lib
vtkRenderingFreeType-7.0-gd.lib
vtkRenderingGL2PS-7.0-gd.lib
vtkRenderingImage-7.0-gd.lib
vtkRenderingLIC-7.0-gd.lib
vtkRenderingLOD-7.0-gd.lib
vtkRenderingLabel-7.0-gd.lib
vtkRenderingOpenGL-7.0-gd.lib
vtkRenderingVolume-7.0-gd.lib
vtkRenderingVolumeOpenGL-7.0-gd.lib
vtkViewsContext2D-7.0-gd.lib
vtkViewsCore-7.0-gd.lib
vtkViewsInfovis-7.0-gd.lib
vtkalglib-7.0-gd.lib
vtkexoIIc-7.0-gd.lib
vtkexpat-7.0-gd.lib
vtkfreetype-7.0-gd.lib
vtkgl2ps-7.0-gd.lib
vtkhdf5-7.0-gd.lib
vtkhdf5_hl-7.0-gd.lib
vtkjpeg-7.0-gd.lib
vtkjsoncpp-7.0-gd.lib
vtklibxml2-7.0-gd.lib
vtkmetaio-7.0-gd.lib
vtkoggtheora-7.0-gd.lib
vtkpng-7.0-gd.lib
vtkproj4-7.0-gd.lib
vtksqlite-7.0-gd.lib
vtksys-7.0-gd.lib
vtktiff-7.0-gd.lib
vtkverdict-7.0-gd.lib
vtkzlib-7.0-gd.lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RELEASE模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcl_common_release.lib
pcl_features_release.lib
pcl_filters_release.lib
pcl_io_ply_release.lib
pcl_io_release.lib
pcl_kdtree_release.lib
pcl_keypoints_release.lib
pcl_octree_release.lib
pcl_outofcore_release.lib
pcl_people_release.lib
pcl_recognition_release.lib
pcl_registration_release.lib
pcl_sample_consensus_release.lib
pcl_search_release.lib
pcl_segmentation_release.lib
pcl_surface_release.lib
pcl_tracking_release.lib
pcl_visualization_release.lib
libboost_atomic-vc120-mt-1_61.lib
libboost_chrono-vc120-mt-1_61.lib
libboost_container-vc120-mt-1_61.lib
libboost_context-vc120-mt-1_61.lib
libboost_coroutine-vc120-mt-1_61.lib
libboost_date_time-vc120-mt-1_61.lib
libboost_exception-vc120-mt-1_61.lib
libboost_filesystem-vc120-mt-1_61.lib
libboost_graph-vc120-mt-1_61.lib
libboost_iostreams-vc120-mt-1_61.lib
libboost_locale-vc120-mt-1_61.lib
libboost_log-vc120-mt-1_61.lib
libboost_log_setup-vc120-mt-1_61.lib
libboost_math_c99-vc120-mt-1_61.lib
libboost_math_c99f-vc120-mt-1_61.lib
libboost_math_c99l-vc120-mt-1_61.lib
libboost_math_tr1-vc120-mt-1_61.lib
libboost_math_tr1f-vc120-mt-1_61.lib
libboost_math_tr1l-vc120-mt-1_61.lib
libboost_mpi-vc120-mt-1_61.lib
libboost_prg_exec_monitor-vc120-mt-1_61.lib
libboost_program_options-vc120-mt-1_61.lib
libboost_random-vc120-mt-1_61.lib
libboost_regex-vc120-mt-1_61.lib
libboost_serialization-vc120-mt-1_61.lib
libboost_signals-vc120-mt-1_61.lib
libboost_system-vc120-mt-1_61.lib
libboost_test_exec_monitor-vc120-mt-1_61.lib
libboost_thread-vc120-mt-1_61.lib
libboost_timer-vc120-mt-1_61.lib
libboost_type_erasure-vc120-mt-1_61.lib
libboost_unit_test_framework-vc120-mt-1_61.lib
libboost_wave-vc120-mt-1_61.lib
libboost_wserialization-vc120-mt-1_61.lib
flann_cpp_s.lib
flann_s.lib
flann.lib
qhull.lib
qhull_p.lib
qhull_r.lib
qhullcpp.lib
qhullstatic.lib
qhullstatic_r.lib
vtkChartsCore-7.0.lib
vtkCommonColor-7.0.lib
vtkCommonComputationalGeometry-7.0.lib
vtkCommonCore-7.0.lib
vtkCommonDataModel-7.0.lib
vtkCommonExecutionModel-7.0.lib
vtkCommonMath-7.0.lib
vtkCommonMisc-7.0.lib
vtkCommonSystem-7.0.lib
vtkCommonTransforms-7.0.lib
vtkDICOMParser-7.0.lib
vtkDomainsChemistry-7.0.lib
vtkFiltersAMR-7.0.lib
vtkFiltersCore-7.0.lib
vtkFiltersExtraction-7.0.lib
vtkFiltersFlowPaths-7.0.lib
vtkFiltersGeneral-7.0.lib
vtkFiltersGeneric-7.0.lib
vtkFiltersGeometry-7.0.lib
vtkFiltersHybrid-7.0.lib
vtkFiltersHyperTree-7.0.lib
vtkFiltersImaging-7.0.lib
vtkFiltersModeling-7.0.lib
vtkFiltersParallel-7.0.lib
vtkFiltersParallelImaging-7.0.lib
vtkFiltersProgrammable-7.0.lib
vtkFiltersSMP-7.0.lib
vtkFiltersSelection-7.0.lib
vtkFiltersSources-7.0.lib
vtkFiltersStatistics-7.0.lib
vtkFiltersTexture-7.0.lib
vtkFiltersVerdict-7.0.lib
vtkGeovisCore-7.0.lib
vtkIOAMR-7.0.lib
vtkIOCore-7.0.lib
vtkIOEnSight-7.0.lib
vtkIOExodus-7.0.lib
vtkIOExport-7.0.lib
vtkIOGeometry-7.0.lib
vtkIOImage-7.0.lib
vtkIOImport-7.0.lib
vtkIOInfovis-7.0.lib
vtkIOLSDyna-7.0.lib
vtkIOLegacy-7.0.lib
vtkIOMINC-7.0.lib
vtkIOMovie-7.0.lib
vtkIONetCDF-7.0.lib
vtkIOPLY-7.0.lib
vtkIOParallel-7.0.lib
vtkIOParallelXML-7.0.lib
vtkIOSQL-7.0.lib
vtkIOVideo-7.0.lib
vtkIOXML-7.0.lib
vtkIOXMLParser-7.0.lib
vtkImagingColor-7.0.lib
vtkImagingCore-7.0.lib
vtkImagingFourier-7.0.lib
vtkImagingGeneral-7.0.lib
vtkImagingHybrid-7.0.lib
vtkImagingMath-7.0.lib
vtkImagingMorphological-7.0.lib
vtkImagingSources-7.0.lib
vtkImagingStatistics-7.0.lib
vtkImagingStencil-7.0.lib
vtkInfovisCore-7.0.lib
vtkInfovisLayout-7.0.lib
vtkInteractionImage-7.0.lib
vtkInteractionStyle-7.0.lib
vtkInteractionWidgets-7.0.lib
vtkNetCDF-7.0.lib
vtkNetCDF_cxx-7.0.lib
vtkParallelCore-7.0.lib
vtkRenderingAnnotation-7.0.lib
vtkRenderingContext2D-7.0.lib
vtkRenderingContextOpenGL-7.0.lib
vtkRenderingCore-7.0.lib
vtkRenderingFreeType-7.0.lib
vtkRenderingGL2PS-7.0.lib
vtkRenderingImage-7.0.lib
vtkRenderingLIC-7.0.lib
vtkRenderingLOD-7.0.lib
vtkRenderingLabel-7.0.lib
vtkRenderingOpenGL-7.0.lib
vtkRenderingVolume-7.0.lib
vtkRenderingVolumeOpenGL-7.0.lib
vtkViewsContext2D-7.0.lib
vtkViewsCore-7.0.lib
vtkViewsInfovis-7.0.lib
vtkalglib-7.0.lib
vtkexoIIc-7.0.lib
vtkexpat-7.0.lib
vtkfreetype-7.0.lib
vtkgl2ps-7.0.lib
vtkhdf5-7.0.lib
vtkhdf5_hl-7.0.lib
vtkjpeg-7.0.lib
vtkjsoncpp-7.0.lib
vtklibxml2-7.0.lib
vtkmetaio-7.0.lib
vtkoggtheora-7.0.lib
vtkpng-7.0.lib
vtkproj4-7.0.lib
vtksqlite-7.0.lib
vtksys-7.0.lib
vtktiff-7.0.lib
vtkverdict-7.0.lib
vtkzlib-7.0.lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意事项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在安装pcl 1.8.0 all-in-one和配置好环境变量之后，需重启下计算机，再进入vs配置&lt;/li&gt;
&lt;li&gt;注意OPENNI的安装路径不要搞错了&lt;/li&gt;
&lt;li&gt;PCL 1.8.0在vs中会有个问题，就是运行的时候会提示：&lt;code&gt;error C4996: &#39;pcl::SAC_SAMPLE_SIZE&#39;: This map is deprecated and is kept only to prevent breaking existing user code. Starting from PCL 1.8.0 model sample size is a protected member of the SampleConsensusModel class&lt;/code&gt;，这个是1.8.0的小毛病，解决方法是：打开项目属性页 -&amp;gt; C/C++ -&amp;gt; 常规 -&amp;gt; SDL检查(设置为否)。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ex1.线性回归</title>
      <link>https://shiweihou.github.io/machinelearning/ex1/</link>
      <pubDate>Sun, 30 Oct 2016 16:10:31 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex1/</guid>
      <description>

&lt;p&gt;从今天开始，开始记录学习机器学习学习过程。目前的学习方法是根据&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Coursera&lt;/a&gt;上Andrew Ng大牛的机器学习课程，据说是同名的斯坦福大学公开课的简化版本，仅介绍基本原理及提供配套的练习，非常适合新手入门。强烈推荐大家去看下视频，讲解的很详细，而且还有配套的演示过程，非常适合新手。&lt;/p&gt;

&lt;p&gt;有关线性回归的具体内容我就不多说了，网上例子有很多，教程有很多，我只谈下在学习过程中自己遇到的一些问题及看法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在学习过程中，参数需要同时更新，不能用更新好的参数去更新下一个参数，例如不能用更新好的 theta0 去更新 theta1.&lt;/li&gt;
&lt;li&gt;如果数据特征变化尺度过大，例如x1在[1,10]范围内，而x2就在[10000,10000000]范围内，就需要进行特征缩放，重新缩放特征的范围到[0, 1]或[-1, 1]。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;顺便贴下第一课的课后作业的答案：&lt;/p&gt;

&lt;h2 id=&#34;computecost-m&#34;&gt;computeCost.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m,
J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computecostmulti-m&#34;&gt;computeCostMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCostMulti(X, y, theta)
%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables
%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m 
    JJ = 0;
    for j = 1 : size(theta,1)
        JJ = JJ + theta(j) * X(i,j);
    end
    JJ = JJ - y(i);
    J = J + JJ ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;featurenormalize-m&#34;&gt;featureNormalize.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.
% You need to set these values correctly
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
% ====================== YOUR CODE HERE ======================
% Instructions: First, for each feature dimension, compute the mean
%               of the feature and subtract it from the dataset,
%               storing the mean value in mu. Next, compute the 
%               standard deviation of each feature and divide
%               each feature by it&#39;s standard deviation, storing
%               the standard deviation in sigma. 
%
%               Note that X is a matrix where each column is a 
%               feature and each row is an example. You need 
%               to perform the normalization separately for 
%               each feature. 
%
% Hint: You might find the &#39;mean&#39; and &#39;std&#39; functions useful.
%       
avg = mean(X);
piancha = std(X);
for row = 1 : size(X,1)
    for col = 1 : size(X,2)
        X_norm(row,col) = (X(row,col) - avg(col)) / piancha(col);
    end
end
mu = avg;
sigma = piancha;
% ============================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescent-m&#34;&gt;gradientDescent.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescentmulti-m&#34;&gt;gradientDescentMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end

end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>