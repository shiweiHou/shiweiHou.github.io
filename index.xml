<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello world!</title>
    <link>https://shiweiHou.github.io/</link>
    <description>Recent content on Hello world!</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 25 Dec 2016 16:47:22 +0800</lastBuildDate>
    <atom:link href="https://shiweiHou.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>lintcode 表达树构造</title>
      <link>https://shiweihou.github.io/lintcode/2016-12-25-03/</link>
      <pubDate>Sun, 25 Dec 2016 16:47:22 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-12-25-03/</guid>
      <description>&lt;p&gt;看到这个题目就感觉和前两天做的“逆波兰表达式”很像，想的方法就是先将表达式转化成逆波兰表达式，因为逆波兰表达式就是后缀表达式，维护一个栈，从头开始遍历后缀表达式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;碰到数值，就生成个结点，然后将结点指针放入栈中&lt;/li&gt;
&lt;li&gt;碰到操作数，就在栈中弹出两个指针作为该操作数的右左子树，接着将指向操作数的指针入栈&lt;/li&gt;

&lt;li&gt;&lt;p&gt;到最后栈中就只会剩下根节点了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition of ExpressionTreeNode:
 * class ExpressionTreeNode {
 * public:
 *     string symbol;
 *     ExpressionTreeNode *left, *right;
 *     ExpressionTreeNode(string symbol) {
 *         this-&amp;gt;symbol = symbol;
 *         this-&amp;gt;left = this-&amp;gt;right = NULL;
 *     }
 * }
 */

class Solution {
public:
    /**
     * @param expression: A string array
     * @return: The root of expression tree
     */
    ExpressionTreeNode* build(vector&amp;lt;string&amp;gt; &amp;amp;expression) {
        // write your code here
        // 首先将表达式转换为逆波兰表达式，也就是表达树的后缀表示形式
        vector&amp;lt;string&amp;gt; nibolan;
        stack&amp;lt;string&amp;gt; operat;
        operat.push(&amp;quot;@&amp;quot;);
        int i = 0;
        while ( i &amp;lt; expression.size()) {
            string s = expression[i];
            if (s != &amp;quot;+&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;-&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;*&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;/&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;(&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;)&amp;quot;) {
                nibolan.push_back(s);

        } else if ( s == &amp;quot;(&amp;quot;) {
            operat.push(s);

        } else if ( s == &amp;quot;)&amp;quot;) {
            string top = operat.top();
            while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; top != &amp;quot;(&amp;quot;) {
                nibolan.push_back(top);
                operat.pop();
                top = operat.top();
            }
            operat.pop();
        } else if ( s == &amp;quot;*&amp;quot; || s == &amp;quot;/&amp;quot;) {
            string top = operat.top();
            while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; (top == &amp;quot;*&amp;quot; || top == &amp;quot;/&amp;quot;) ) {
                nibolan.push_back(top);
                operat.pop();
                top = operat.top();
            }
            operat.push(s);
        } else {
           string top = operat.top();
            while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; top != &amp;quot;(&amp;quot;) {
                nibolan.push_back(top);
                operat.pop();
                top = operat.top();
            }
            operat.push(s); 
        }
        ++i;
    }
    string top = operat.top();
    while (top != &amp;quot;@&amp;quot; ) {
        nibolan.push_back(top);
        operat.pop();
        top = operat.top();
    }
    //从前往后，根据后缀表达式构建表达树,维护一个栈，栈中存放着树的指针，如果是操作符，就从栈中弹出两个指针作为它的右左子树，如果是数字，就直接入栈，最后栈中只会留下根指针
    stack&amp;lt;ExpressionTreeNode*&amp;gt; st;
    i = 0;
    while (i &amp;lt; nibolan.size()) {
        string s = nibolan[i];
        ExpressionTreeNode * node = new ExpressionTreeNode (s);
        if (s == &amp;quot;*&amp;quot; || s == &amp;quot;/&amp;quot; || s == &amp;quot;+&amp;quot; || s == &amp;quot;-&amp;quot;) {
            ExpressionTreeNode* right = st.top();
            st.pop();
            ExpressionTreeNode* left = st.top();
            st.pop();
            node-&amp;gt;left = left;
            node-&amp;gt;right = right;
        }
        st.push(node);
        ++i;
    }
    if (st.empty()) return NULL;
    return st.top();

}
};
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>lintcode 用栈模拟汉诺塔问题</title>
      <link>https://shiweihou.github.io/lintcode/2016-12-25-02/</link>
      <pubDate>Sun, 25 Dec 2016 16:41:24 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-12-25-02/</guid>
      <description>&lt;p&gt;我们知道汉诺塔问题是个经典的递归问题，思路就是有三根柱子 A B C，将N个按大小排好的碟子从A移动到C上，每次只移动一个并且只能将小的放在大的上面。将N-1个碟子利用柱子B从A放在B上，再将第N个碟子从A移动到C上，然后将剩下的N-1个碟子从B再移动到A上，重复这个过程，直到所有的碟子都移动完毕。根据Lintcode提供的接口，我们只需要补充完整就好了，在做的过程中也遇到了一些小问题，在代码里都注释了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Tower {
public:
    // create three towers (i from 0 to 2)
    Tower(int i) {}

    // Add a disk into this tower
    void add(int d) {
        if (!disks.empty() &amp;amp;&amp;amp; disks.top() &amp;lt;= d) {
            printf(&amp;quot;Error placing disk %d&amp;quot;, d);
        } else {
            disks.push(d);
        }
    }

    // @param t a tower
    // Move the top disk of this tower to the top of t.
    void moveTopTo(Tower &amp;amp;t) {
        // Write your code here
        //不明白加上这句if (disks.top() &amp;gt; t.disks.top())为什么错误
        //1是没有考虑到disks是private，不能直接由对象调用，2是没有判断t的disnks是不是空的，所以才出错
        //if (disks.top() &amp;gt; t.disks.top()) ;
        stack&amp;lt;int&amp;gt; s = t.getDisks();
        if (!s.empty() &amp;amp;&amp;amp; disks.top() &amp;gt; s.top()) ;
        t.add(disks.top());
        disks.pop();

    }

    // @param n an integer
    // @param destination a tower
    // @param buffer a tower
    // Move n Disks from this tower to destination by buffer tower
    void moveDisks(int n, Tower &amp;amp;destination, Tower &amp;amp;buffer) {
        // Write your code here
        if (n == 0) return ;
        else if (n == 1) moveTopTo(destination);
        else {
            moveDisks(n-1,buffer,destination);
            moveTopTo(destination);
            buffer.moveDisks(n-1,destination,*this);
        }

    }

    stack&amp;lt;int&amp;gt; getDisks() {
        return disks;
    }

private:
    stack&amp;lt;int&amp;gt; disks;
};
/**
 * Your Tower object will be instantiated and called as such:
 * vector&amp;lt;Tower&amp;gt; towers;
 * for (int i = 0; i &amp;lt; 3; i++) towers.push_back(Tower(i));
 * for (int i = n - 1; i &amp;gt;= 0; i--) towers[0].add(i);
 * towers[0].moveDisks(n, towers[2], towers[1]);
 * print towers[0], towers[1], towers[2]
*/
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>lintcode 矩阵的之字型遍历</title>
      <link>https://shiweihou.github.io/lintcode/2016-12-25-01/</link>
      <pubDate>Sun, 25 Dec 2016 16:33:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-12-25-01/</guid>
      <description>&lt;p&gt;题目要求：给一个矩阵，按照之字型进行遍历，&lt;a href=&#34;http://www.lintcode.com/zh-cn/problem/matrix-zigzag-traversal/&#34;&gt;题目链接&lt;/a&gt;
思路：其实仔细想一想，就是先斜着往下遍历，再斜着往上遍历，就是这个过程，主要是判断好临界条件，是要往右走还是往下走
代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
    /**
     * @param matrix: a matrix of integers
     * @return: a vector of integers
     */
    vector&amp;lt;int&amp;gt; printZMatrix(vector&amp;lt;vector&amp;lt;int&amp;gt; &amp;gt; &amp;amp;matrix) {
        // write your code here
        int row = matrix.size();
        int col = matrix[0].size();
        vector&amp;lt;int&amp;gt; ret (row * col, 0);
        int r = 0;
        int c = 0;
        int i = 0;
        int all = row * col;
        ret[i++] = matrix[0][0];
        while (i &amp;lt; all) {
            // 斜上走到顶
            while (i &amp;lt; all &amp;amp;&amp;amp; r - 1 &amp;gt;= 0 &amp;amp;&amp;amp; c + 1 &amp;lt; col) {
                ret[i++] = matrix[--r][++c];
            }
            // 横右走一步，不可横右走时竖下走一步
            if (i &amp;lt; all &amp;amp;&amp;amp; c + 1 &amp;lt; col) {
                ret[i++] = matrix[r][++c];
            } else if (i &amp;lt; all &amp;amp;&amp;amp; r + 1 &amp;lt; row) {
                ret[i++] = matrix[++r][c];
            }
            // 斜下走到底
            while (i &amp;lt; all &amp;amp;&amp;amp; r + 1 &amp;lt; row &amp;amp;&amp;amp; c - 1 &amp;gt;= 0) {
                ret[i++] = matrix[++r][--c];
            }
            // 竖下走一步，不可竖下时横右走一步
            if (i &amp;lt; all &amp;amp;&amp;amp; r + 1 &amp;lt; row) {
                ret[i++] = matrix[++r][c];
            } else if (i &amp;lt; all &amp;amp;&amp;amp; c + 1 &amp;lt; col) {
                ret[i++] = matrix[r][++c];
            }
        }
        return ret;
    }
};
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LintCode 将表达式转换为逆波兰表达式</title>
      <link>https://shiweihou.github.io/lintcode/2016-12-22-01/</link>
      <pubDate>Thu, 22 Dec 2016 20:55:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/lintcode/2016-12-22-01/</guid>
      <description>&lt;p&gt;问题描述：
    给定一个表达式字符串数组，返回该表达式的逆波兰表达式。&lt;/p&gt;

&lt;p&gt;问题分析：首先要搞明白什么是逆波兰表达式，&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E7%A4%BA%E6%B3%95&#34;&gt;wiki&lt;/a&gt;上面有很详细的介绍，题目意思就是将我们平常习惯的数学表达式表示成计算机所能理解的式子。因为人类可以很自然的理解例如&lt;code&gt;1 + 2 * 3&lt;/code&gt;这样的式子，但计算机没有办法，计算机需要将其转换为&lt;code&gt;123*+&lt;/code&gt;这样的形式才可以。&lt;/p&gt;

&lt;p&gt;前面有一道题是将逆波兰表达式转换为中缀表达式，这道题刚好反过来。其实如果我们首先先自己在草稿纸上自己将中缀表达式转为逆波兰表达式的过程演算一遍，就很自然的明白转换规则：假设我们有一个&lt;code&gt;vector ret&lt;/code&gt;用来专门存答案，有一个&lt;code&gt;stack opera&lt;/code&gt;用来存操作符（&lt;code&gt;+ - * / （&lt;/code&gt;） ），另字符串&lt;code&gt;ch&lt;/code&gt;为表达式&lt;code&gt;expression&lt;/code&gt;中的元素，规则如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;假如&lt;code&gt;ch&lt;/code&gt;为数字，那么很自然的，将其放入&lt;code&gt;ret&lt;/code&gt;中&lt;/li&gt;
&lt;li&gt;假如&lt;code&gt;ch&lt;/code&gt;为操作符，那么：

&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果&lt;code&gt;ch == &amp;quot;(&amp;quot;&lt;/code&gt;,那么将&lt;code&gt;ch&lt;/code&gt;放入到&lt;code&gt;opera&lt;/code&gt;中；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果&lt;code&gt;ch == &amp;quot;)&amp;quot;&lt;/code&gt;,那么将&lt;code&gt;opera&lt;/code&gt;中最顶层的&lt;code&gt;（&lt;/code&gt;上面的所有操作符放入&lt;code&gt;ret&lt;/code&gt;中&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果&lt;code&gt;ch
是四则运算符&lt;/code&gt;,那么根据此时&lt;code&gt;opera&lt;/code&gt;中已有的字符的优先级，如果&lt;code&gt;ch&lt;/code&gt;大于此时&lt;code&gt;opera&lt;/code&gt;顶层操作符的优先级，那么&lt;code&gt;ch&lt;/code&gt;放入到&lt;code&gt;opera&lt;/code&gt;中，如果不是，那么将&lt;code&gt;opera&lt;/code&gt;中的运算符弹出放入&lt;code&gt;ret&lt;/code&gt;中，直到碰到第一个大于&lt;code&gt;ch&lt;/code&gt;的优先级的运算符或者碰到第一个&lt;code&gt;（&lt;/code&gt;或者走到头。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们首先放进去一个字符@，表示优先级最低或者表示栈底
代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
    /**
     * @param expression: A string array
     * @return: The Reverse Polish notation of this expression
     */
    vector&amp;lt;string&amp;gt; convertToRPN(vector&amp;lt;string&amp;gt; &amp;amp;expression) {
        // write your code here
        vector&amp;lt;string&amp;gt; nibolan;
        stack&amp;lt;string&amp;gt; operat;
        operat.push(&amp;quot;@&amp;quot;);
        int i = 0;
        while ( i &amp;lt; expression.size()) {
            string s = expression[i];
            if (s != &amp;quot;+&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;-&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;*&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;/&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;(&amp;quot; &amp;amp;&amp;amp; s != &amp;quot;)&amp;quot;) {
                nibolan.push_back(s);

            } else if ( s == &amp;quot;(&amp;quot;) {
                operat.push(s);

            } else if ( s == &amp;quot;)&amp;quot;) {
                string top = operat.top();
                while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; top != &amp;quot;(&amp;quot;) {
                    nibolan.push_back(top);
                    operat.pop();
                    top = operat.top();
                }
                operat.pop();
            } else if ( s == &amp;quot;*&amp;quot; || s == &amp;quot;/&amp;quot;) {
                string top = operat.top();
                while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; (top == &amp;quot;*&amp;quot; || top == &amp;quot;/&amp;quot;) ) {
                    nibolan.push_back(top);
                    operat.pop();
                    top = operat.top();
                }
                operat.push(s);
            } else {
               string top = operat.top();
                while (top != &amp;quot;@&amp;quot; &amp;amp;&amp;amp; top != &amp;quot;(&amp;quot;) {
                    nibolan.push_back(top);
                    operat.pop();
                    top = operat.top();
                }
                operat.push(s); 
            }
            ++i;
        }
        string top = operat.top();
        while (top != &amp;quot;@&amp;quot; ) {
            nibolan.push_back(top);
            operat.pop();
            top = operat.top();
        }

        return nibolan;
    }
};
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex9-10.The end</title>
      <link>https://shiweihou.github.io/machinelearning/exend/</link>
      <pubDate>Sat, 17 Dec 2016 15:03:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/exend/</guid>
      <description>&lt;p&gt;第九周和第十周没有再讲具体的算法了，而是介绍了一些机器学习变种的算法。第九周介绍了如何在非常非常大的数据下进行机器学习，以及流算法，第十周则介绍了流水线工作，类似于模块化，将一个大的机器学习任务分解成几个小部分，然后分析哪部分可以进行改进，哪部分不需要花费太多的人力物力物改进。&lt;/p&gt;

&lt;p&gt;总之，感觉正学习的渐入佳境，突然就没了，总有一种意犹未尽的感觉。机器学习只是比较有兴趣，然而现在我用到的机会却不是很多，希望可以继续学习下去，获得更多的知识。&lt;/p&gt;

&lt;p&gt;再次感谢Andrew Ng的无私奉献，还有Coursera平台提供了这次机会，再次感谢！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>C&#43;&#43;按行读取文本字符串</title>
      <link>https://shiweihou.github.io/cplusplus/C&#43;&#43;%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%AD%97%E7%AC%A6%E4%B8%B2/</link>
      <pubDate>Tue, 13 Dec 2016 16:55:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/cplusplus/C&#43;&#43;%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%AD%97%E7%AC%A6%E4%B8%B2/</guid>
      <description>&lt;p&gt;我们有一个文本文件，文件信息都是按行存储的，并且每行都包含很多字符串，例如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;you are my girl!
This is number 123456!
Do you like me? aha
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类似于这样，我们有时候可能希望不仅仅读取文本里的字符串，还希望能够按行读取，因为有时候文本里的内容是相关的，即每行的信息大同小异，但每行间又会有点小区别，或者我们希望处理某特定一行的数字等等。这时我们希望可以每次读取一行，然后把每行的字符串都拆分出来，我们就可以按照下面的操作：
std::fstream
C++ 有一个头文件&lt;code&gt;&amp;lt;fstream&amp;gt;&lt;/code&gt;,里面有三个类成员 &lt;code&gt;fstream ifstream ofstream&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ifstream：定义读取的文件，进行读操作，而无法进行写操作&lt;/li&gt;
&lt;li&gt;ofstream：定义要写的文件，进行写操作，而无法进行读操作&lt;/li&gt;
&lt;li&gt;fstream：定义读写文件，既可以进行读操作，也可以进行写操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一行一行的读取文件内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;fstream&amp;gt;

std::string fileName = &amp;quot;line.ply&amp;quot;;
std::ifstream input_file(fileName.c_str());
std::string line;
if (input_file.is_open()) {
    while (getline(input_file, line)) {
        cout &amp;lt;&amp;lt; line &amp;lt;&amp;lt; endl; // 文件每一行的数据都保存在line中
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上式中&lt;code&gt;getline&lt;/code&gt;函数，就将每一行的内容都放入line中，直到读取到末尾。&lt;/p&gt;

&lt;p&gt;那么现在我们得到每行的数据了，如何得到每行单独的字符串呢，这时候就要用到&lt;code&gt;sstream&lt;/code&gt;，它和&lt;code&gt;fstream&lt;/code&gt;类似，不过前者是对文件进行操作，是文件IO，后者是对string对象操作，就像string是一个IO流一样。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;istringstream：从string读取数据&lt;/li&gt;
&lt;li&gt;ostringstream：向string写入数据&lt;/li&gt;
&lt;li&gt;stringstream：对string既可以写，也可以读&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对上面得到的一行数据进行操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include&amp;lt;sstream&amp;gt;
std::istringstream text(line);
std::string content;
while (text &amp;gt;&amp;gt; content) {
    do something with content.//这时我们就将一行的数据拆分成单个的字符串，就可以进行操作了
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex8.Anomaly Detection and Recommender Systems</title>
      <link>https://shiweihou.github.io/machinelearning/ex8/</link>
      <pubDate>Sat, 03 Dec 2016 18:03:25 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex8/</guid>
      <description>

&lt;p&gt;这节课主要讲了如何进行异常检测，介绍了推荐系统的基本概念和如何搭建一个推荐系统，都是一些基础概念，比较简单。
已经第九周了，还有两周的课就结束了，↖加油(^ω^)↗&lt;/p&gt;

&lt;h2 id=&#34;estimategaussian-m&#34;&gt;estimateGaussian.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [mu sigma2] = estimateGaussian(X)
%ESTIMATEGAUSSIAN This function estimates the parameters of a 
%Gaussian distribution using the data in X
%   [mu sigma2] = estimateGaussian(X), 
%   The input X is the dataset with each n-dimensional data point in one row
%   The output is an n-dimensional vector mu, the mean of the data set
%   and the variances sigma^2, an n x 1 vector
% 
% Useful variables
[m, n] = size(X);
% You should return these values correctly
mu = zeros(n, 1);
sigma2 = zeros(n, 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the mean of the data and the variances
%               In particular, mu(i) should contain the mean of
%               the data for the i-th feature and sigma2(i)
%               should contain variance of the i-th feature.
%
% X 是m行n列，sum（X)将每一列的和加起来然后除去行数m得到一个1*n维的行向量，进行转置，得到列向量
mu = (sum(X)/m)&#39;;

mu2 = mu&#39;;%将mu变成原来1*n维的行向量[1 2 3 .. n]
%mu2(ones(m,1),:)是将一个1*n维的行向量变成m*n的矩阵，即复制了m次，此时mu2就变成了
%    [u1,u2,u3,...,un
%     u1,u2,u3,...,un
%     .
%     .
%     u1,u2,u3,...,un
sigma2 = (sum((X - mu2(ones(m,1),:)).^2) / m )&#39;;
% =============================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;selectthreshold-m&#34;&gt;selectThreshold.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [bestEpsilon bestF1] = selectThreshold(yval, pval)
%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting
%outliers
%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best
%   threshold to use for selecting outliers based on the results from a
%   validation set (pval) and the ground truth (yval).
%

bestEpsilon = 0;
bestF1 = 0;
F1 = 0;

stepsize = (max(pval) - min(pval)) / 1000;
for epsilon = min(pval):stepsize:max(pval)

    % ====================== YOUR CODE HERE ======================
    % Instructions: Compute the F1 score of choosing epsilon as the
    %               threshold and place the value in F1. The code at the
    %               end of the loop will compare the F1 score for this
    %               choice of epsilon and set it to be the best epsilon if
    %               it is better than the current choice of epsilon.
    %               
    % Note: You can use predictions = (pval &amp;lt; epsilon) to get a binary vector
    %       of 0&#39;s and 1&#39;s of the outlier predictions

    predictions = (pval &amp;lt; epsilon);
    fp = sum((predictions == 1) &amp;amp; (yval == 0));
    tp = sum((predictions == 1) &amp;amp; (yval == 1));
    fn = sum((predictions == 0) &amp;amp; (yval == 1));
    prec = tp / (tp + fp);
    rec = tp / (tp + fn);
    F1 = 2 * prec * rec / (prec + rec);
    % =============================================================
    if F1 &amp;gt; bestF1
       bestF1 = F1;
       bestEpsilon = epsilon;
    end
end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;coficostfunc-m&#34;&gt;cofiCostFunc.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...
                              num_features, lambda)
%COFICOSTFUNC Collaborative filtering cost function
%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...
%   num_features, lambda) returns the cost and gradient for the
%   collaborative filtering problem.
%

% Unfold the U and W matrices from params
X = reshape(params(1:num_movies*num_features), num_movies, num_features);
Theta = reshape(params(num_movies*num_features+1:end), ...
                num_users, num_features);


% You need to return the following values correctly
J = 0;
X_grad = zeros(size(X));
Theta_grad = zeros(size(Theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost function and gradient for collaborative
%               filtering. Concretely, you should first implement the cost
%               function (without regularization) and make sure it is
%               matches our costs. After that, you should implement the 
%               gradient and use the checkCostFunction routine to check
%               that the gradient is correct. Finally, you should implement
%               regularization.
%
% Notes: X - num_movies  x num_features matrix of movie features
%        Theta - num_users  x num_features matrix of user features
%        Y - num_movies x num_users matrix of user ratings of movies
%        R - num_movies x num_users matrix, where R(i, j) = 1 if the 
%            i-th movie was rated by the j-th user
%
% You should set the following variables correctly:
%
%        X_grad - num_movies x num_features matrix, containing the 
%                 partial derivatives w.r.t. to each element of X
%        Theta_grad - num_users x num_features matrix, containing the 
%                     partial derivatives w.r.t. to each element of Theta
%

t1 = X*Theta&#39;;
t1 = t1 - Y;
t1 = t1.^2;
J = sum(sum(R.*t1)) / 2;
% with regularization J
J = J + lambda / 2 * ( sum( sum (Theta.^2) )  + sum( sum (X.^2) ));
X_grad = R.*(X*Theta&#39; - Y) * Theta;
Theta_grad = (R.*(X*Theta&#39; - Y))&#39; * X;
% with regularization   
X_grad = X_grad + lambda * X;
Theta_grad = Theta_grad + lambda * Theta;
% =============================================================
grad = [X_grad(:); Theta_grad(:)];

end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex7.K-means Clustering and Principal Component Analysis</title>
      <link>https://shiweihou.github.io/machinelearning/ex7/</link>
      <pubDate>Tue, 29 Nov 2016 11:18:44 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex7/</guid>
      <description>

&lt;p&gt;这节课主要讲了非监督学习的两个：K-means 和 Principal Component Analysis。K-means算法思想很简单，就是给你一堆数据，你需要在这无序的数据中将类似的数据聚合起来，分成K类，具体要分几类，可以通过 Elbow method （肘部法则）或者根据实际需求来确定。PCA算法通俗意义来讲，是降维操作。如果有个数据特征维数很大，例如100000维，其实在这么多的特种中，很多特征都是由相互联系的，通过PCA算法，就可以将原来维数很大数据特征，降低到我们容易进行处理的维数。&lt;/p&gt;

&lt;p&gt;PCA有几个好处： 降低存储容量，通过降维，提高学习算法运行速度。特别的，通过降维到2D或者3D，我们可以显示的观察特征，利于处理。但是如果想要通过PCA去降维来避免学习算法的过拟合问题，是不推荐的，或者不可取的。&lt;/p&gt;

&lt;h2 id=&#34;findclosestcentroids-m&#34;&gt;findClosestCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;m = size(X,1);
for i = 1 : m
    %二维数组，第i行用X(i,:)表示，不是X(i)
    dis = sum((X(i,:) - centroids(1,:)).^2);
    idx(i) = 1;
    for k = 1 : K
        dis2 = sum((X(i,:) - centroids(k,:)).^2);
        if  dis &amp;gt;= dis2
            dis = dis2;
            idx(i) = k;
        end
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computecentroids-m&#34;&gt;computeCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for k = 1 : K
    centroid = zeros(1,n);
    Ck = 0;
    for i = 1 : m
        if idx(i) == k
            centroid = centroid + X(i,:);
            Ck = Ck + 1;
        end
    end
    centroid = centroid / Ck;
    centroids(k,:) = centroid;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kmeansinitcentroids-m&#34;&gt;kMeansInitCentroids.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;randidx = randperm(size(X,1));
centroids = X(randidx(1:K),:);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pca-m&#34;&gt;pca.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;eigenvector = X&#39;* X / m;
[U,S,~] = svd(eigenvector);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;projectdata-m&#34;&gt;projectData.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;U_reduce = U(:,1:K);
Z = X * U_reduce;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;recoverdata-m&#34;&gt;recoverData.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;X_rec = Z * U(:,1:K)&#39;;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex6.Support Vector Machines</title>
      <link>https://shiweihou.github.io/machinelearning/ex6/</link>
      <pubDate>Fri, 25 Nov 2016 20:52:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex6/</guid>
      <description>

&lt;p&gt;这节讲到了SVM，Andrew Ng大神主要简单介绍了下核函数和SVM一些参数的影响。因为Ng大牛介绍的比较简单，所以看完自己又去网上百度了下，系统了了解了下。拙见就不在这发了，大家有兴趣的可以自己去百度。其实SVM就是个分类器，利用特定的核函数将在原空间维度线性不可分问题映射到高维，使之变成线性可分。另外学习这节课的时候又对偏差和方差有点迷，在知乎上找到一篇答案，比较优秀，链接在此：&lt;a href=&#34;https://www.zhihu.com/question/27068705&#34;&gt;机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;gaussian-kernel&#34;&gt;Gaussian Kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%很简单，就是按照公式计算高斯核内积，难度0
sim = -sum( (x1 - x2).^2 ) /  2 / sigma^2;
sim = exp(sim);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dataset3params-m&#34;&gt;dataset3Params.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%选择出最好的一对 C sigma
C_final = C;
sigma_final = sigma;
min_error = size(yval,1);
% C 和 sigma 所有的可能
para = [0.01 0.03 0.1 0.3 1 3 10 30];
m = size(para,2);
% 将 C 和 sigma 所有的可能都训练测试一遍，找到效果最好的那对
for i = 1 : m
    for j = 1 : m
        C = para(i);
        sigma = para(j);
        model = svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));
        pre = svmPredict(model,Xval);
        cur_error = mean(double(pre ~= yval));
        if cur_error &amp;lt; min_error
            min_error = cur_error;
            C_final = C;
            sigma_final = sigma;
        end
    end
end
C = C_final;
sigma = sigma_final;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;processemail-m&#34;&gt;processEmail.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%就是找到单词出现的下标位置，然后已列向量存储下来
for i = 1 : length(vocabList)
    if strcmp(str, vocabList{i}) == 1
        word_indices = [word_indices ; i];
        break;
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;emailfeatures-m&#34;&gt;emailFeatures.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%这个就更简单了，简单的说就是将所有出现的单词所在的下标位置，赋值1
for i = 1 : size(word_indices,1)
    x(word_indices(i)) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在已经学习到第七周了，发现越高深的算法，其实实现起来越简单，因为现有的函数库都将其实质内容都包含隐藏起来了，你只需要实现相应的输入参数就可以了。说明发明一个算法是最难的，但学会用一个算法，就太简单了。
另在原有的代码有一个小bug，就是没办法显示Example Dataset 2的边界线，是visualizeBoundary.m出了点问题，课程助教也给了解决方法，及时将原有的&lt;code&gt;contour(X1, X2, vals, [0 0], &#39;Color&#39;, &#39;b&#39;);&lt;/code&gt;改为&lt;code&gt;contour(X1, X2, vals, [1 1], &#39;b&#39;);&lt;/code&gt;就可以了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pcl_boundary</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_boundary/</link>
      <pubDate>Fri, 18 Nov 2016 13:41:18 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_boundary/</guid>
      <description>&lt;p&gt;PCL 里又找到一个牛逼的函数：BoundaryEstimation，用来发现三维点云的边界的。因为一直在做三维重建，一开始的想法是将三维点云投影到二维上，接着将二维的点云进行网格划分，根据落在网格内的点的数量设置为灰度值，这样我们就可以得到一个灰度图。接着就是利用图像处理中的方法，先进行二值化，然后用canny算子找边界，用hough变换找直线，最后再反投射到三维上。后来试了下，这个方法不仅复杂，而且进行两次三维到二维之间的变化，误差太大。后来在PCL里找到BoundaryEstimation这个类，能够自动的找到边界，很强大，但最好也是在二维下做，意思是先找到面，在面上找边界。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void LasFunction::boundaryFind(void) {
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();
    pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt;::Ptr tree = boost::shared_ptr&amp;lt;pcl::search::Search&amp;lt;pcl::PointXYZ&amp;gt; &amp;gt;(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;);
    // 计算法向量，因为边界点查找需要用到法向量
    pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;::Ptr normals(new pcl::PointCloud&amp;lt;pcl::Normal&amp;gt;);
    pcl::NormalEstimation&amp;lt;pcl::PointXYZ, pcl::Normal&amp;gt; normal_estimator;
    normal_estimator.setSearchMethod(tree);
    normal_estimator.setInputCloud(cloud);
    normal_estimator.setKSearch(30);
    normal_estimator.compute(*normals);

    pcl::PointCloud&amp;lt;pcl::Boundary&amp;gt; boundaries;
    pcl::BoundaryEstimation&amp;lt;pcl::PointXYZ, pcl::Normal, pcl::Boundary&amp;gt; est;
    est.setInputCloud(cloud);
    est.setInputNormals(normals);
    est.setSearchMethod(pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;::Ptr(new pcl::search::KdTree&amp;lt;pcl::PointXYZ&amp;gt;));
    //50效果比较好，这个数值的意思我的理解是根据点周围这么多个点来进行判断我们找到的点究竟是不是边界点
    est.setKSearch(100);
    est.compute(boundaries);
    if (boundaries.points.empty()) std::cout &amp;lt;&amp;lt; &amp;quot;error&amp;quot; &amp;lt;&amp;lt; std::endl;
    else {
        pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr boundaryPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
        for (int i = 0; i &amp;lt; cloud-&amp;gt;points.size(); ++i) {
        //当初在这卡了好久，因为虽然找到了边界点，但无法直接输出，它里面是uint8_t类型的，如果是边界点，它保存1，如果不是，保存0.若想在计算机里显式的显示出来，需要进行强制类型转换，变成int型才可以
            uint8_t  x = boundaries.points[i].boundary_point;
            int a = static_cast&amp;lt;int&amp;gt;(x);
            if (1 == a) boundaryPoints-&amp;gt;push_back(cloud-&amp;gt;points[i]);
        }
        std::cout &amp;lt;&amp;lt; boundaryPoints-&amp;gt;points.size() &amp;lt;&amp;lt; std::endl;
        std::cout &amp;lt;&amp;lt; &amp;quot;boundary is ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        radiusFilter(boundaryPoints);
        pcl::io::savePCDFile(&amp;quot;./line9/boundaryPlane.pcd&amp;quot;, *boundaryPoints);
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总之，这个类给了我很大的方便，PCL果然在一直进步，很牛逼的一个库，需要我继续学习下去&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PCL SampleConsensusMode</title>
      <link>https://shiweihou.github.io/pointcloud/pcl_ransac2/</link>
      <pubDate>Fri, 18 Nov 2016 13:19:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/pointcloud/pcl_ransac2/</guid>
      <description>

&lt;p&gt;前面那篇用PCL里的ransac模型进行面提取，其实严格意义来讲并不是真正的RANSAC类，而是利用的SACSegmentation类，然后在类。里面用SACMODEL_PLANE模型进行面提取。后来又找到PCL里真正的RANSAC函数 SampleConsensusMode，遂用这个实现了一下，发现效果要比前面的好一些，并且不仅在提面上效果好，在提线的表现上也非常好。&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelplane&#34;&gt;SampleConsensusModelPlane&lt;/h2&gt;

&lt;p&gt;这个用来提取面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelPlane(void) {
    // ransac 提取出来一个最大的平面
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    // 读取文件，将点云数据放入cloud中
    cloud = readFile();
    // 原始点云数量，用来判断何时停止面的提取
    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    // 当剩下的点云数量比原始点云数量大于10%时时，继续进行面的提取
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.1 * rawPoints) {

        //确定提取模型为面，点云类型要和前面的点云类型一致
        pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_p(new pcl::SampleConsensusModelPlane&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_p);
        //这个Threshold的意思是，同一平面的点相邻距离不能超过0.1m，或者是已0.1m为基准进行搜索，不能超过它。我试过用1 2等参数进行提取，发现提取出来的不仅仅是一个面了，还有高度，就像是一个立方体一样。所以最好小一些，可以根据具体的点云数据进行设置
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        //这个函数的意思是将inputPoints中，前面提取到的面（索引保留在了inliers中了），复制到点云out中，这时out中存的就是前面提取的面的点
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //建立析取器，因为我们需要不停的迭代，需要将上次提取出来的面从原来的点云中去除
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        //因为前面用到的索引是vector类型的，而析取器要求的是PointIndices，所以只要新建个，然后将上面的索引复制给它就行了
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        //只有设置为true，才可以将包含在inlier中的下标的点从原来的点云中去除
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);

        std::cout &amp;lt;&amp;lt; &amp;quot;ok.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;SampleModelPlane&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了可以提取面之外，这个函数还有个变形，就是提取直线，效果也是非常好的&lt;/p&gt;

&lt;h2 id=&#34;sampleconsensusmodelline&#34;&gt;SampleConsensusModelLine&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;bool SampleConsensusModelLine(void) {
    std::vector&amp;lt;int&amp;gt; inliers;
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr cloud(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr out(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    cloud = readFile();

    int rawPoints = cloud-&amp;gt;points.size();
    pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;::Ptr inputPoints(new pcl::PointCloud&amp;lt;pcl::PointXYZ&amp;gt;);
    inputPoints = cloud;
    int count = 1;
    std::multimap&amp;lt;double, double&amp;gt; mapKb;
    //和上面的函数几乎没有什么区别，只是将SampleConsensusModelPlane变成SampleConsensusModelLine
    while (inputPoints-&amp;gt;points.size() &amp;gt; 0.05 * rawPoints) {
        pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;::Ptr modle_l(new pcl::SampleConsensusModelLine&amp;lt;pcl::PointXYZ&amp;gt;(inputPoints));
        pcl::RandomSampleConsensus&amp;lt;pcl::PointXYZ&amp;gt; ransac(modle_l);
        ransac.setDistanceThreshold(0.1);
        ransac.computeModel();
        ransac.getInliers(inliers);
        pcl::copyPointCloud&amp;lt;pcl::PointXYZ&amp;gt;(*inputPoints, inliers, *out);

        //析取
        pcl::ExtractIndices&amp;lt;pcl::PointXYZ&amp;gt; eifilter(false);
        pcl::PointIndices::Ptr inlier(new pcl::PointIndices);
        inlier-&amp;gt;indices = inliers;
        eifilter.setNegative(true);
        eifilter.setInputCloud(inputPoints);
        eifilter.setIndices(inlier);
        eifilter.filter(*inputPoints);


        std::cout &amp;lt;&amp;lt; &amp;quot;ok2.&amp;quot; &amp;lt;&amp;lt; std::endl;
        std::string fileName = &amp;quot;./line1/SampleModelLine&amp;quot;, houzhui = &amp;quot;.pcd&amp;quot;;
        pcl::PCDWriter w;
        w.write&amp;lt;pcl::PointXYZ&amp;gt;(fileName + std::to_string(count) + houzhui, *out);
        ++count;
    }

    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为用的是1.8版本的PCL，里面实现了好多很需要的函数，所以功能还是很强大，几乎你能想到的都有，只是因为是通用的，所以可能在精度准确度上没有那么高，需要开发者自己再做进一步调整&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex5.Regularized Linear Regression and Bias v.s.Variance</title>
      <link>https://shiweihou.github.io/machinelearning/ex5/</link>
      <pubDate>Thu, 17 Nov 2016 21:15:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex5/</guid>
      <description>

&lt;p&gt;这个练习其实很简单，基本上都是以前练习的代码，然后让你再重写一遍。这次编程作业的主要作用就是让我们直观的看到不同大小的训练集及不同大小的lambda对拟合效果的影响。具体有以下几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature太少或者维数过低，会造成高偏差(high bias)，即欠拟合,解决方法是增加feature数，可以是维数增加或者feature数增加（polynomial features)或者降低lambda（正则化项）,feature数增加的前提示增加的feature必须和已有feature是相互独立的，不然没有任何意义，只能增加维数了&lt;/li&gt;
&lt;li&gt;类似于上条，如果feature过多或者feature的维数过高，会造成高方差(high variance),即过拟合。解决方法是减少feature数，将不是相互独立的feature去掉，或者降低维数，或者增加正则化项，降低维数对theta参数的影响。还有一个很有用的方法是增加训练集样本，来解决过拟合问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;regularized-linear-regression-cost-function-and-gradient&#34;&gt;Regularized linear regression cost function and Gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%非矩阵形式
for i = 1 : m
    h = theta(1) * X(i,1) + theta(2) * X(i,2);
    J = J +  (h - y(i))^2 / 2 / m;
end
J = J + lambda / 2 / m * theta(2)^2;
%矩阵形式
J = sum((X * theta - y).^2) / 2 / m + lambda / 2 /m * (sum(theta.^2) - theta(1)^2); 

theta1 = theta;
theta1(1) = 0;
grad = (X&#39; * (X * theta - y))/m + lambda / m * theta1;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;learning-curves&#34;&gt;Learning curves&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% ---------------------- Sample Solution ----------------------
for i = 1 : m
    theta = trainLinearReg(X(1:i,:), y(1:i,:), lambda);
    [error_train(i),grad] = linearRegCostFunction(X(1:i,:), y(1:i,:), theta, 0);
    [error_val(i),grad] = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个步骤中变得很笨，这个步骤是要让我们形式化的看到不同的训练集大小对误差的影响，我在计算训练误差的时候还好，但在计算error_val交叉集误差的时候，也学训练集那样只用Xval和yval的前i个样本，其实是要全部的样本。&lt;/p&gt;

&lt;h2 id=&#34;polynomial-regression&#34;&gt;Polynomial regression&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : p
    X_poly(:,i) = X.^i;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分主要让我们观察不同的feature polynomial对误差的影响，意思是原来的训练集X只有一列，然后我们根据p的值，另外增加p-1列，其中第2列的值为第一列值的平方，第3列的值为第一列值的立方，以此类推&lt;/p&gt;

&lt;h2 id=&#34;selecting-λ-using-a-cross-validation-set&#34;&gt;Selecting λ using a cross validation set&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : length(lambda_vec)
    lambda = lambda_vec(i);
    theta = trainLinearReg(X, y, lambda);
    error_train(i) = linearRegCostFunction(X, y, theta, 0);
    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分就和Learning curves这部分很像，所不同的是Learning curves部分看训练集大小对误差的影响，这次呢，是看不同的lambda λ对误差的影响，训练集和交叉集大小是固定的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex4.Neural Networks Learning</title>
      <link>https://shiweihou.github.io/machinelearning/ex4/</link>
      <pubDate>Wed, 09 Nov 2016 10:49:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex4/</guid>
      <description>

&lt;p&gt;Neural Networks Learning：实现最简单的一个神经网络学习系统，实现反向传播和正向传播，并利用数值计算误差来检测神经网络算法是否可行，或者说cost function是否计算正确。&lt;/p&gt;

&lt;h2 id=&#34;feedforward-and-cost-function&#34;&gt;Feedforward and cost function&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through the
%               following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%         variable J. After implementing Part 1, you can verify that your
%         cost function computation is correct by verifying the cost
%         computed in ex4.m
a1 = [ones(m,1) X]; % 5000 * 401
Z2 = a1 * Theta1&#39;;  % 5000 * hidden_layer
a2 = sigmoid(Z2);   % sigmoid
a2 = [ones(size(a2,1),1) a2]; % 5000 * (hidden_layer + 1)
Z3 = a2 * Theta2&#39;;            % 5000 * 10
a3 = sigmoid(Z3);
[max_a3, index] = max(a3,[],2);

I = eye(num_labels); 
Y = zeros(m, num_labels);  % 5000 * 10
for i = 1:m
    Y(i,:) = I(y(i),:);  %  让每一个样例第y(i)列变成1，即将样本输出变成行向量，而不是列向量，每一行为1的列的值即为样本的输出
end
%J = sum(sum((-Y).*log(a3) - (1-Y).*log(1-a3),2))/m;
JJ = 0;
for i = 1 : m   
   JJ = JJ + sum( -1*Y(i,:).* log(a3(i,:)) - (1-Y(i,:)).*log(1-a3(i,:))); %对所有的样本进行相加，每个样本计算J，最后加起来
end
J = JJ / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-cost-function&#34;&gt;Regularized cost function&lt;/h2&gt;

&lt;p&gt;就是在前面计算出来的J的基础上，加上正则化项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%要去掉theta(1),第一个theta不需要参加
theta1 = Theta1(:,2:size(Theta1,2));
theta2 = Theta2(:,2:size(Theta2,2));
reg = lambda * ( sum ( sum ( theta1.^2) ) + sum ( sum ( theta2.^2) ));
reg = reg / 2 / m;
J = J + reg;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sigmoid-gradient&#34;&gt;Sigmoid gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function g = sigmoidGradient(z)
%SIGMOIDGRADIENT returns the gradient of the sigmoid function
%evaluated at z
%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function
%   evaluated at z. This should work regardless if z is a matrix or a
%   vector. In particular, if z is a vector or matrix, you should return
%   the gradient for each element.

g = zeros(size(z));
g = sigmoid(z).* (1 - sigmoid(z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sigma3 = a3-Y;
sigma2 = (sigma3*Theta2).*sigmoidGradient([ones(size(Z2, 1), 1) Z2]);
sigma2 = sigma2(:, 2:end);

delta_1 = (sigma2&#39;*a1);
delta_2 = (sigma3&#39;*a2);

p1 = (lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];
p2 = (lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];
Theta1_grad = delta_1./m + p1;
Theta2_grad = delta_2./m + p2;

grad = [Theta1_grad(:) ; Theta2_grad(:)];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;神经网络算法比较简单，就是计算过程稍微复杂了点，特别是牵涉到里面的矩阵运算的时候，比较麻烦。第一次做的时候感觉无从下手，后来又做了一遍，才感觉稍微懂点。思路都懂，就是写出来比较困难，还需努力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex3.Multi-class Classification and Neural Networks</title>
      <link>https://shiweihou.github.io/machinelearning/ex3/</link>
      <pubDate>Thu, 03 Nov 2016 19:56:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex3/</guid>
      <description>

&lt;p&gt;Multi-class Classification&lt;/p&gt;

&lt;p&gt;多分类，其实和前面的二分类，即Logistics regression差不多，只不过因为分类结果有很多个，对于特定的输入，会产生多个输出，在这多个输出里面找到概率最大的那个输出，即为分类答案。&lt;/p&gt;

&lt;h2 id=&#34;lrcostfunction-m&#34;&gt;lrCostFunction.m&lt;/h2&gt;

&lt;p&gt;和第二个练习的一样，就是指导文档里要求我们向量化，即矩阵化求解。前面的练习我们已经做到了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J = (0-y).*log(J1) - (1-y).* log (1-J1);
J = sum(J);
J = J / m;
J = J + (lambda / 2 / m) * (sum(theta.^2) - theta(1)^2);
% 求 Gradient 
grad = X&#39; *  (J1 - y ) / m;
tmp = theta;
tmp(1) = 0;
grad = grad + (lambda / m) * tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;one-vs-all-classification&#34;&gt;One-vs-all Classification&lt;/h2&gt;

&lt;p&gt;其实多分类也是一个“二分类”问题，只不过每次我们都是选取其中概率最大的那个作为输出“1”，其余的都作为“0”。&lt;strong&gt;oneVsAll.m&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for c = 1 : num_labels
    %yc = (y == c);
    initial_theta = zeros(n + 1, 1);
    options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
    [theta] = ...
         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...
                 initial_theta, options);
    all_theta(c,:) = theta&#39;;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;predictonevsall-m&#34;&gt;predictOneVsAll.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% max函数，得到tmp矩阵中每一行最大的那个列下标
% 在本题中，列下标即为分类结果
[maximum, index] = max(tmp, [], 2);
% 如果下标是10，我们就用0表示
index(index == 10) = 0;
p = index;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;这节课Andrew还简单的介绍了下神经网络，并且给出了如何利用神经网络来分类。编程题中Andrew已经给好了训练好的Theta值，只需要我们能根据训练好的Theta值和输入的X值，得到Y值就行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
%加上第一项1（bias unit)，因为X是m * n的，每个样本的特征在一行，所以加上的就是 m*1的一列，即在每一行的前面加上了个1
X = [ones(m, 1) X];
for i = 1 : m
    XX = X(i:i,:);
    z2 = Theta1 * XX&#39;;
    z2 = sigmoid(z2);
    %row = size(z2,1)
    %加上第一项 1，因为z2是 n*1的，即第i个样本的z2都是单独一列，是个列向量，因此需要加上一行1，即增加一行1
    z22 = [ones(1,1);z2];
    z3 = Theta2 * z22;
    % a3 即为输出结果
    a3 = sigmoid(z3);
    [maxInd, index] = max(a3,[],1);
    if index == 0
        index = 10;
    end
    p(i) = index;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在每一层网络中计算的时候，都要注意要加上一项1作为bias unit&lt;/li&gt;
&lt;li&gt;注意在前面一层输出和Theta积计算完之后，不要忘记做sigmoid操作&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ex2.Logistics Regression</title>
      <link>https://shiweihou.github.io/machinelearning/ex2/</link>
      <pubDate>Thu, 03 Nov 2016 18:34:49 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex2/</guid>
      <description>

&lt;p&gt;第二次编程作业，逻辑回归，其实可以理解为二分类问题。&lt;/p&gt;

&lt;h2 id=&#34;sigmoid-function&#34;&gt;sigmoid function：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% 迭代实现
[row, col] = size(z); 
for i = 1:row
    for j = 1:col
        g(i,j) = 1 / (1 + exp(-z(i,j)));
    end
end

% 非迭代实现
g = 1.0 ./ (1.0 + exp(-z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cost-function-and-gradient&#34;&gt;Cost function and Gradient：&lt;/h2&gt;

&lt;p&gt;一开始我是用迭代做的，后来学习了第四课之后，我又回来将迭代的版本改为矩阵版本，看起来确实清晰了很多，以后尽量都用矩阵形式来做（好像这样也更符合matlab的要求）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 迭代版本
theta1 = theta&#39;;
[row,col] = size(theta1);
% 求Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    grad(j,1) = J2;
end
% 求 J，cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是矩阵版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J1 = sigmoid(X*theta);
% 求 J， cost function
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-logistic-regression&#34;&gt;Regularized logistic regression：&lt;/h2&gt;

&lt;p&gt;为什么要用到正则化呢？是因为有时候特征项我们选取的过多，有时候会出现过拟合问题。那么，一旦出现过拟合问题的时候，就会发生训练模型对训练集拟合非常好，但对新的数据就拟合偏差很大。这个时候，我们就需要用正则化项来惩罚参数，防止其过大，从而避免过拟合问题。同样，我一开始写的是迭代过程，后来又被我改成矩阵形式了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta1 = theta&#39;;
[row,col] = size(theta1);
% 求 Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    %对第一个参数 theta0 不进行正则化
    if j &amp;gt;= 2
        grad(j,1) = J2 + (lambda / m ) * theta(j);
    else
        grad(j,1) = J2;
    end
end
%求 J， cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
% 对第一个参数 theta0 不进行正则化
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;矩阵形式，和上面的logistics regression矩阵形式差不多，只需要注意不要对第一项进行处理即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
grad = grad + (lambda / m ) * theta;
grad(1) = grad(1) - (lambda / m ) * theta(1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对了，还有个predict函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : m
    XX = X(i:i,:);
    pp = sigmoid(XX*theta);
    if pp &amp;gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>