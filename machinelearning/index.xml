<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MachineLearnings on Hello world!</title>
    <link>https://shiweiHou.github.io/machinelearning/</link>
    <description>Recent content in MachineLearnings on Hello world!</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 25 Nov 2016 20:52:54 +0800</lastBuildDate>
    <atom:link href="https://shiweiHou.github.io/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ex6.Support Vector Machines</title>
      <link>https://shiweihou.github.io/machinelearning/ex6/</link>
      <pubDate>Fri, 25 Nov 2016 20:52:54 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex6/</guid>
      <description>

&lt;p&gt;这节讲到了SVM，Andrew Ng大神主要简单介绍了下核函数和SVM一些参数的影响。因为Ng大牛介绍的比较简单，所以看完自己又去网上百度了下，系统了了解了下。拙见就不在这发了，大家有兴趣的可以自己去百度。其实SVM就是个分类器，利用特定的核函数将在原空间维度线性不可分问题映射到高维，使之变成线性可分。另外学习这节课的时候又对偏差和方差有点迷，在知乎上找到一篇答案，比较优秀，链接在此：&lt;a href=&#34;https://www.zhihu.com/question/27068705&#34;&gt;机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;gaussian-kernel&#34;&gt;Gaussian Kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%很简单，就是按照公式计算高斯核内积，难度0
sim = -sum( (x1 - x2).^2 ) /  2 / sigma^2;
sim = exp(sim);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dataset3params-m&#34;&gt;dataset3Params.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%选择出最好的一对 C sigma
C_final = C;
sigma_final = sigma;
min_error = size(yval,1);
% C 和 sigma 所有的可能
para = [0.01 0.03 0.1 0.3 1 3 10 30];
m = size(para,2);
% 将 C 和 sigma 所有的可能都训练测试一遍，找到效果最好的那对
for i = 1 : m
    for j = 1 : m
        C = para(i);
        sigma = para(j);
        model = svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));
        pre = svmPredict(model,Xval);
        cur_error = mean(double(pre ~= yval));
        if cur_error &amp;lt; min_error
            min_error = cur_error;
            C_final = C;
            sigma_final = sigma;
        end
    end
end
C = C_final;
sigma = sigma_final;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;processemail-m&#34;&gt;processEmail.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%就是找到单词出现的下标位置，然后已列向量存储下来
for i = 1 : length(vocabList)
    if strcmp(str, vocabList{i}) == 1
        word_indices = [word_indices ; i];
        break;
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;emailfeatures-m&#34;&gt;emailFeatures.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%这个就更简单了，简单的说就是将所有出现的单词所在的下标位置，赋值1
for i = 1 : size(word_indices,1)
    x(word_indices(i)) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在已经学习到第七周了，发现越高深的算法，其实实现起来越简单，因为现有的函数库都将其实质内容都包含隐藏起来了，你只需要实现相应的输入参数就可以了。说明发明一个算法是最难的，但学会用一个算法，就太简单了。
另在原有的代码有一个小bug，就是没办法显示Example Dataset 2的边界线，是visualizeBoundary.m出了点问题，课程助教也给了解决方法，及时将原有的&lt;code&gt;contour(X1, X2, vals, [0 0], &#39;Color&#39;, &#39;b&#39;);&lt;/code&gt;改为&lt;code&gt;contour(X1, X2, vals, [1 1], &#39;b&#39;);&lt;/code&gt;就可以了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex5.Regularized Linear Regression and Bias v.s.Variance</title>
      <link>https://shiweihou.github.io/machinelearning/ex5/</link>
      <pubDate>Thu, 17 Nov 2016 21:15:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex5/</guid>
      <description>

&lt;p&gt;这个练习其实很简单，基本上都是以前练习的代码，然后让你再重写一遍。这次编程作业的主要作用就是让我们直观的看到不同大小的训练集及不同大小的lambda对拟合效果的影响。具体有以下几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature太少或者维数过低，会造成高偏差(high bias)，即欠拟合,解决方法是增加feature数，可以是维数增加或者feature数增加（polynomial features)或者降低lambda（正则化项）,feature数增加的前提示增加的feature必须和已有feature是相互独立的，不然没有任何意义，只能增加维数了&lt;/li&gt;
&lt;li&gt;类似于上条，如果feature过多或者feature的维数过高，会造成高方差(high variance),即过拟合。解决方法是减少feature数，将不是相互独立的feature去掉，或者降低维数，或者增加正则化项，降低维数对theta参数的影响。还有一个很有用的方法是增加训练集样本，来解决过拟合问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;regularized-linear-regression-cost-function-and-gradient&#34;&gt;Regularized linear regression cost function and Gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;%非矩阵形式
for i = 1 : m
    h = theta(1) * X(i,1) + theta(2) * X(i,2);
    J = J +  (h - y(i))^2 / 2 / m;
end
J = J + lambda / 2 / m * theta(2)^2;
%矩阵形式
J = sum((X * theta - y).^2) / 2 / m + lambda / 2 /m * (sum(theta.^2) - theta(1)^2); 

theta1 = theta;
theta1(1) = 0;
grad = (X&#39; * (X * theta - y))/m + lambda / m * theta1;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;learning-curves&#34;&gt;Learning curves&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% ---------------------- Sample Solution ----------------------
for i = 1 : m
    theta = trainLinearReg(X(1:i,:), y(1:i,:), lambda);
    [error_train(i),grad] = linearRegCostFunction(X(1:i,:), y(1:i,:), theta, 0);
    [error_val(i),grad] = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个步骤中变得很笨，这个步骤是要让我们形式化的看到不同的训练集大小对误差的影响，我在计算训练误差的时候还好，但在计算error_val交叉集误差的时候，也学训练集那样只用Xval和yval的前i个样本，其实是要全部的样本。&lt;/p&gt;

&lt;h2 id=&#34;polynomial-regression&#34;&gt;Polynomial regression&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : p
    X_poly(:,i) = X.^i;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分主要让我们观察不同的feature polynomial对误差的影响，意思是原来的训练集X只有一列，然后我们根据p的值，另外增加p-1列，其中第2列的值为第一列值的平方，第3列的值为第一列值的立方，以此类推&lt;/p&gt;

&lt;h2 id=&#34;selecting-λ-using-a-cross-validation-set&#34;&gt;Selecting λ using a cross validation set&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : length(lambda_vec)
    lambda = lambda_vec(i);
    theta = trainLinearReg(X, y, lambda);
    error_train(i) = linearRegCostFunction(X, y, theta, 0);
    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一部分就和Learning curves这部分很像，所不同的是Learning curves部分看训练集大小对误差的影响，这次呢，是看不同的lambda λ对误差的影响，训练集和交叉集大小是固定的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex4.Neural Networks Learning</title>
      <link>https://shiweihou.github.io/machinelearning/ex4/</link>
      <pubDate>Wed, 09 Nov 2016 10:49:07 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex4/</guid>
      <description>

&lt;p&gt;Neural Networks Learning：实现最简单的一个神经网络学习系统，实现反向传播和正向传播，并利用数值计算误差来检测神经网络算法是否可行，或者说cost function是否计算正确。&lt;/p&gt;

&lt;h2 id=&#34;feedforward-and-cost-function&#34;&gt;Feedforward and cost function&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through the
%               following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%         variable J. After implementing Part 1, you can verify that your
%         cost function computation is correct by verifying the cost
%         computed in ex4.m
a1 = [ones(m,1) X]; % 5000 * 401
Z2 = a1 * Theta1&#39;;  % 5000 * hidden_layer
a2 = sigmoid(Z2);   % sigmoid
a2 = [ones(size(a2,1),1) a2]; % 5000 * (hidden_layer + 1)
Z3 = a2 * Theta2&#39;;            % 5000 * 10
a3 = sigmoid(Z3);
[max_a3, index] = max(a3,[],2);

I = eye(num_labels); 
Y = zeros(m, num_labels);  % 5000 * 10
for i = 1:m
    Y(i,:) = I(y(i),:);  %  让每一个样例第y(i)列变成1，即将样本输出变成行向量，而不是列向量，每一行为1的列的值即为样本的输出
end
%J = sum(sum((-Y).*log(a3) - (1-Y).*log(1-a3),2))/m;
JJ = 0;
for i = 1 : m   
   JJ = JJ + sum( -1*Y(i,:).* log(a3(i,:)) - (1-Y(i,:)).*log(1-a3(i,:))); %对所有的样本进行相加，每个样本计算J，最后加起来
end
J = JJ / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-cost-function&#34;&gt;Regularized cost function&lt;/h2&gt;

&lt;p&gt;就是在前面计算出来的J的基础上，加上正则化项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%要去掉theta(1),第一个theta不需要参加
theta1 = Theta1(:,2:size(Theta1,2));
theta2 = Theta2(:,2:size(Theta2,2));
reg = lambda * ( sum ( sum ( theta1.^2) ) + sum ( sum ( theta2.^2) ));
reg = reg / 2 / m;
J = J + reg;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sigmoid-gradient&#34;&gt;Sigmoid gradient&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function g = sigmoidGradient(z)
%SIGMOIDGRADIENT returns the gradient of the sigmoid function
%evaluated at z
%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function
%   evaluated at z. This should work regardless if z is a matrix or a
%   vector. In particular, if z is a vector or matrix, you should return
%   the gradient for each element.

g = zeros(size(z));
g = sigmoid(z).* (1 - sigmoid(z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sigma3 = a3-Y;
sigma2 = (sigma3*Theta2).*sigmoidGradient([ones(size(Z2, 1), 1) Z2]);
sigma2 = sigma2(:, 2:end);

delta_1 = (sigma2&#39;*a1);
delta_2 = (sigma3&#39;*a2);

p1 = (lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];
p2 = (lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];
Theta1_grad = delta_1./m + p1;
Theta2_grad = delta_2./m + p2;

grad = [Theta1_grad(:) ; Theta2_grad(:)];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;神经网络算法比较简单，就是计算过程稍微复杂了点，特别是牵涉到里面的矩阵运算的时候，比较麻烦。第一次做的时候感觉无从下手，后来又做了一遍，才感觉稍微懂点。思路都懂，就是写出来比较困难，还需努力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ex3.Multi-class Classification and Neural Networks</title>
      <link>https://shiweihou.github.io/machinelearning/ex3/</link>
      <pubDate>Thu, 03 Nov 2016 19:56:35 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex3/</guid>
      <description>

&lt;p&gt;Multi-class Classification&lt;/p&gt;

&lt;p&gt;多分类，其实和前面的二分类，即Logistics regression差不多，只不过因为分类结果有很多个，对于特定的输入，会产生多个输出，在这多个输出里面找到概率最大的那个输出，即为分类答案。&lt;/p&gt;

&lt;h2 id=&#34;lrcostfunction-m&#34;&gt;lrCostFunction.m&lt;/h2&gt;

&lt;p&gt;和第二个练习的一样，就是指导文档里要求我们向量化，即矩阵化求解。前面的练习我们已经做到了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J = (0-y).*log(J1) - (1-y).* log (1-J1);
J = sum(J);
J = J / m;
J = J + (lambda / 2 / m) * (sum(theta.^2) - theta(1)^2);
% 求 Gradient 
grad = X&#39; *  (J1 - y ) / m;
tmp = theta;
tmp(1) = 0;
grad = grad + (lambda / m) * tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;one-vs-all-classification&#34;&gt;One-vs-all Classification&lt;/h2&gt;

&lt;p&gt;其实多分类也是一个“二分类”问题，只不过每次我们都是选取其中概率最大的那个作为输出“1”，其余的都作为“0”。&lt;strong&gt;oneVsAll.m&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for c = 1 : num_labels
    %yc = (y == c);
    initial_theta = zeros(n + 1, 1);
    options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
    [theta] = ...
         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...
                 initial_theta, options);
    all_theta(c,:) = theta&#39;;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;predictonevsall-m&#34;&gt;predictOneVsAll.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% max函数，得到tmp矩阵中每一行最大的那个列下标
% 在本题中，列下标即为分类结果
[maximum, index] = max(tmp, [], 2);
% 如果下标是10，我们就用0表示
index(index == 10) = 0;
p = index;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;这节课Andrew还简单的介绍了下神经网络，并且给出了如何利用神经网络来分类。编程题中Andrew已经给好了训练好的Theta值，只需要我们能根据训练好的Theta值和输入的X值，得到Y值就行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
%加上第一项1（bias unit)，因为X是m * n的，每个样本的特征在一行，所以加上的就是 m*1的一列，即在每一行的前面加上了个1
X = [ones(m, 1) X];
for i = 1 : m
    XX = X(i:i,:);
    z2 = Theta1 * XX&#39;;
    z2 = sigmoid(z2);
    %row = size(z2,1)
    %加上第一项 1，因为z2是 n*1的，即第i个样本的z2都是单独一列，是个列向量，因此需要加上一行1，即增加一行1
    z22 = [ones(1,1);z2];
    z3 = Theta2 * z22;
    % a3 即为输出结果
    a3 = sigmoid(z3);
    [maxInd, index] = max(a3,[],1);
    if index == 0
        index = 10;
    end
    p(i) = index;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在每一层网络中计算的时候，都要注意要加上一项1作为bias unit&lt;/li&gt;
&lt;li&gt;注意在前面一层输出和Theta积计算完之后，不要忘记做sigmoid操作&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ex2.Logistics Regression</title>
      <link>https://shiweihou.github.io/machinelearning/ex2/</link>
      <pubDate>Thu, 03 Nov 2016 18:34:49 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex2/</guid>
      <description>

&lt;p&gt;第二次编程作业，逻辑回归，其实可以理解为二分类问题。&lt;/p&gt;

&lt;h2 id=&#34;sigmoid-function&#34;&gt;sigmoid function：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% 迭代实现
[row, col] = size(z); 
for i = 1:row
    for j = 1:col
        g(i,j) = 1 / (1 + exp(-z(i,j)));
    end
end

% 非迭代实现
g = 1.0 ./ (1.0 + exp(-z));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cost-function-and-gradient&#34;&gt;Cost function and Gradient：&lt;/h2&gt;

&lt;p&gt;一开始我是用迭代做的，后来学习了第四课之后，我又回来将迭代的版本改为矩阵版本，看起来确实清晰了很多，以后尽量都用矩阵形式来做（好像这样也更符合matlab的要求）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 迭代版本
theta1 = theta&#39;;
[row,col] = size(theta1);
% 求Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    grad(j,1) = J2;
end
% 求 J，cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是矩阵版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J1 = sigmoid(X*theta);
% 求 J， cost function
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regularized-logistic-regression&#34;&gt;Regularized logistic regression：&lt;/h2&gt;

&lt;p&gt;为什么要用到正则化呢？是因为有时候特征项我们选取的过多，有时候会出现过拟合问题。那么，一旦出现过拟合问题的时候，就会发生训练模型对训练集拟合非常好，但对新的数据就拟合偏差很大。这个时候，我们就需要用正则化项来惩罚参数，防止其过大，从而避免过拟合问题。同样，我一开始写的是迭代过程，后来又被我改成矩阵形式了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta1 = theta&#39;;
[row,col] = size(theta1);
% 求 Gradient
for j = 1:col
    J2 = 0;
    for i = 1:m
        XX = X(i:i,:);
        JJ = XX * theta;
        J1 = 1 / (1 + exp(-JJ));
        J2 = J2 + (J1 - y(i))*X(i,j);
    end
    J2 = J2 / m;
    %对第一个参数 theta0 不进行正则化
    if j &amp;gt;= 2
        grad(j,1) = J2 + (lambda / m ) * theta(j);
    else
        grad(j,1) = J2;
    end
end
%求 J， cost function
for i = 1:m
    XX = X(i:i,:);
    JJ = XX * theta;
    J1 = 1 / (1 + exp(-JJ));
    J2 = ( -y(i)*log(J1) )  - ( (1-y(i))*log(1-J1) );
    J = J + J2;
end
% 对第一个参数 theta0 不进行正则化
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;矩阵形式，和上面的logistics regression矩阵形式差不多，只需要注意不要对第一项进行处理即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% 求 J
J1 = sigmoid(X*theta);
J2 = -(y&#39;) * log(J1);
J3 = (1 - y&#39;) * log( 1 - J1);
J = J2 - J3;
J = J / m;
J = J + (lambda / 2 / m ) * ( sum(theta.^2) - theta(1)^2);
% 求 Gradient
g1 = J1 - y;
g2 = X&#39; * g1;
grad = g2 / m;
grad = grad + (lambda / m ) * theta;
grad(1) = grad(1) - (lambda / m ) * theta(1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对了，还有个predict函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1 : m
    XX = X(i:i,:);
    pp = sigmoid(XX*theta);
    if pp &amp;gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ex1.线性回归</title>
      <link>https://shiweihou.github.io/machinelearning/ex1/</link>
      <pubDate>Sun, 30 Oct 2016 16:10:31 +0800</pubDate>
      
      <guid>https://shiweihou.github.io/machinelearning/ex1/</guid>
      <description>

&lt;p&gt;从今天开始，开始记录学习机器学习学习过程。目前的学习方法是根据&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Coursera&lt;/a&gt;上Andrew Ng大牛的机器学习课程，据说是同名的斯坦福大学公开课的简化版本，仅介绍基本原理及提供配套的练习，非常适合新手入门。强烈推荐大家去看下视频，讲解的很详细，而且还有配套的演示过程，非常适合新手。&lt;/p&gt;

&lt;p&gt;有关线性回归的具体内容我就不多说了，网上例子有很多，教程有很多，我只谈下在学习过程中自己遇到的一些问题及看法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在学习过程中，参数需要同时更新，不能用更新好的参数去更新下一个参数，例如不能用更新好的 theta0 去更新 theta1.&lt;/li&gt;
&lt;li&gt;如果数据特征变化尺度过大，例如x1在[1,10]范围内，而x2就在[10000,10000000]范围内，就需要进行特征缩放，重新缩放特征的范围到[0, 1]或[-1, 1]。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;顺便贴下第一课的课后作业的答案：&lt;/p&gt;

&lt;h2 id=&#34;computecost-m&#34;&gt;computeCost.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m,
J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computecostmulti-m&#34;&gt;computeCostMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function J = computeCostMulti(X, y, theta)
%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables
%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y
% Initialize some useful values
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
for i = 1 : m 
    JJ = 0;
    for j = 1 : size(theta,1)
        JJ = JJ + theta(j) * X(i,j);
    end
    JJ = JJ - y(i);
    J = J + JJ ^ 2;
end
J = J / (2 * m);
% =========================================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;featurenormalize-m&#34;&gt;featureNormalize.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.
% You need to set these values correctly
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
% ====================== YOUR CODE HERE ======================
% Instructions: First, for each feature dimension, compute the mean
%               of the feature and subtract it from the dataset,
%               storing the mean value in mu. Next, compute the 
%               standard deviation of each feature and divide
%               each feature by it&#39;s standard deviation, storing
%               the standard deviation in sigma. 
%
%               Note that X is a matrix where each column is a 
%               feature and each row is an example. You need 
%               to perform the normalization separately for 
%               each feature. 
%
% Hint: You might find the &#39;mean&#39; and &#39;std&#39; functions useful.
%       
avg = mean(X);
piancha = std(X);
for row = 1 : size(X,1)
    for col = 1 : size(X,2)
        X_norm(row,col) = (X(row,col) - avg(col)) / piancha(col);
    end
end
mu = avg;
sigma = piancha;
% ============================================================
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescent-m&#34;&gt;gradientDescent.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradientdescentmulti-m&#34;&gt;gradientDescentMulti.m&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
% ====================== YOUR CODE HERE ======================
% Instructions: Perform a single gradient step on the parameter vector
%               theta. 
%
% Hint: While debugging, it can be useful to print out the values
%       of the cost function (computeCost) and gradient here.
%
% ============================================================
% Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    theta1 = theta;
    for j = 1:2
    J = 0;
    for i = 1 : m
        J = J + (theta(1) * X(i,1) + theta(2) * X(i,2) - y(i)) * X(i,j);
    end
    theta1(j) = theta(j) - alpha * J / m;
    end
    theta = theta1;
end

end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>